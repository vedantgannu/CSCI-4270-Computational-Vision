{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1648866727418,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "MsZRu2dd5J_D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from google.colab import drive, files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4859,
     "status": "ok",
     "timestamp": 1648867137784,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "Q8qDVurwGK-8",
    "outputId": "25280ecd-ff13-45df-f2a0-891dcff52b10"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24223,
     "status": "ok",
     "timestamp": 1648860510577,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "lUW2UTSV5T5T",
    "outputId": "b61eec3d-076f-48ff-b0a5-a27a792c0759"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1648867288155,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "69KcvXk3ML6T"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "def prepare_dataset(train_valid_test_option, resize_shape=None, batch_size=32, shuffle_data=False):\n",
    "  #Aggregate the data\n",
    "  ROOTPATH = \".\"\n",
    "  grass_annotations = pd.DataFrame(data = {\"image\": os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"grass\"))\n",
    "                                          ,\"label\":len(os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"grass\")))*[0]})\n",
    "  ocean_annotations = pd.DataFrame(data = {\"image\": os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"ocean\"))\n",
    "                                          ,\"label\":len(os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"ocean\")))*[1]})\n",
    "  redcarpet_annotations = pd.DataFrame(data = {\"image\": os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"redcarpet\"))\n",
    "                                          ,\"label\":len(os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"redcarpet\")))*[2]})\n",
    "  road_annotations = pd.DataFrame(data = {\"image\": os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"road\"))\n",
    "                                          ,\"label\":len(os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"road\")))*[3]})\n",
    "  wheatfield_annotations = pd.DataFrame(data = {\"image\": os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"wheatfield\"))\n",
    "                                          ,\"label\":len(os.listdir(os.path.join(ROOTPATH, train_valid_test_option, \"wheatfield\")))*[4]})\n",
    "\n",
    "  grass_dataset = CustomDataset(\n",
    "            img_dir = os.path.join(ROOTPATH, train_valid_test_option, \"grass\")\n",
    "          , annotations = grass_annotations\n",
    "          , transform = Resize(resize_shape) if resize_shape else None\n",
    "  )\n",
    "  ocean_dataset = CustomDataset(\n",
    "            img_dir = os.path.join(ROOTPATH, train_valid_test_option, \"ocean\")\n",
    "          , annotations = ocean_annotations\n",
    "          , transform = Resize(resize_shape) if resize_shape else None\n",
    "  )\n",
    "  redcarpet_dataset = CustomDataset(\n",
    "            img_dir = os.path.join(ROOTPATH, train_valid_test_option, \"redcarpet\")\n",
    "          , annotations = redcarpet_annotations\n",
    "          , transform = Resize(resize_shape) if resize_shape else None\n",
    "  )\n",
    "  road_dataset = CustomDataset(\n",
    "            img_dir = os.path.join(ROOTPATH, train_valid_test_option, \"road\")\n",
    "          , annotations = road_annotations\n",
    "          , transform = Resize(resize_shape) if resize_shape else None\n",
    "  )\n",
    "  wheatfield_dataset = CustomDataset(\n",
    "            img_dir = os.path.join(ROOTPATH, train_valid_test_option, \"wheatfield\")\n",
    "          , annotations = wheatfield_annotations\n",
    "          , transform = Resize(resize_shape) if resize_shape else None\n",
    "  )\n",
    "  aggregate_dataset = ConcatDataset([grass_dataset, ocean_dataset, redcarpet_dataset\\\n",
    "                                    ,road_dataset, wheatfield_dataset])\n",
    "  final_dataloader = DataLoader(aggregate_dataset, batch_size=batch_size, shuffle=shuffle_data)\n",
    "  return final_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1648867291260,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "uwcZDNYnMWC5",
    "outputId": "f136bf8e-5aad-4181-8a61-98ddc67e0ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataloader initialized: 3500\n",
      "Validation Dataloader initialized:  750\n",
      "Testing Dataloader initialized:  750\n"
     ]
    }
   ],
   "source": [
    "#Retreiving datasets for NN training and validation set testing\n",
    "training_dataloader = prepare_dataset(\"train\", resize_shape=None, batch_size=32, shuffle_data=True)\n",
    "print(\"Training Dataloader initialized:\", len(training_dataloader.dataset))\n",
    "validation_dataloader = prepare_dataset(\"valid\", resize_shape=None, batch_size=32, shuffle_data=True)\n",
    "print(\"Validation Dataloader initialized: \", len(validation_dataloader.dataset))\n",
    "testing_dataloader = prepare_dataset(\"test\", resize_shape=None, batch_size=32, shuffle_data=True)\n",
    "print(\"Testing Dataloader initialized: \", len(testing_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1648867294364,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "G2FRwUYx5bNW",
    "outputId": "1886c5b6-083c-458c-cd13-f2b36a835808"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device).float(), y.to(device).to(torch.int64)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(loss)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            #return train_loss/batches\n",
    "    return train_loss/batches\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    pred_out = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device).to(torch.int64)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()#Accumulating the loss per batch\n",
    "            predicted_class = pred.argmax(1)\n",
    "            correct += (predicted_class == y).type(torch.float).sum().item()\n",
    "            compare = torch.cat((torch.reshape(predicted_class, (len(predicted_class), 1))\n",
    "                                                          , torch.reshape(y, (len(y), 1))), 1)\n",
    "            pred_out.append(compare)\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, torch.cat(pred_out, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1648867296771,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "rlsWTam25b3B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1648867298728,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "NbgPfQEzOKkW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_Conv, self).__init__()\n",
    "        #Images are R=240 by C=360\n",
    "        self.conv_stack = nn.Sequential(\n",
    "           nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #  Resulting image should be 60*90\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #Padding preserves shape, but 2 max pools divides dims by 4\n",
    "            nn.Linear(60*90*32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc_stack(self.conv_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4-pce43OMP3",
    "outputId": "e6bf10cc-fdda-41df-a93b-98dfa4218e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 4.981244  [   32/ 3500]\n",
      "loss: 1.595293  [  352/ 3500]\n",
      "loss: 1.473764  [  672/ 3500]\n",
      "loss: 1.378121  [  992/ 3500]\n",
      "loss: 1.213845  [ 1312/ 3500]\n",
      "loss: 1.357765  [ 1632/ 3500]\n",
      "loss: 1.620574  [ 1952/ 3500]\n",
      "loss: 1.469261  [ 2272/ 3500]\n",
      "loss: 1.225049  [ 2592/ 3500]\n",
      "loss: 1.254465  [ 2912/ 3500]\n",
      "loss: 1.135642  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 1.127390 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.052873  [   32/ 3500]\n",
      "loss: 1.157790  [  352/ 3500]\n",
      "loss: 1.351038  [  672/ 3500]\n",
      "loss: 0.873365  [  992/ 3500]\n",
      "loss: 1.212297  [ 1312/ 3500]\n",
      "loss: 0.909985  [ 1632/ 3500]\n",
      "loss: 1.170891  [ 1952/ 3500]\n",
      "loss: 1.325726  [ 2272/ 3500]\n",
      "loss: 0.934268  [ 2592/ 3500]\n",
      "loss: 1.303783  [ 2912/ 3500]\n",
      "loss: 0.947412  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.270181 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.178568  [   32/ 3500]\n",
      "loss: 1.110567  [  352/ 3500]\n",
      "loss: 1.116441  [  672/ 3500]\n",
      "loss: 0.624168  [  992/ 3500]\n",
      "loss: 0.834600  [ 1312/ 3500]\n",
      "loss: 0.858544  [ 1632/ 3500]\n",
      "loss: 0.914909  [ 1952/ 3500]\n",
      "loss: 0.996145  [ 2272/ 3500]\n",
      "loss: 0.896293  [ 2592/ 3500]\n",
      "loss: 0.719601  [ 2912/ 3500]\n",
      "loss: 1.126245  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.012934 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.916552  [   32/ 3500]\n",
      "loss: 0.651494  [  352/ 3500]\n",
      "loss: 0.666687  [  672/ 3500]\n",
      "loss: 0.653013  [  992/ 3500]\n",
      "loss: 0.881694  [ 1312/ 3500]\n",
      "loss: 0.743756  [ 1632/ 3500]\n",
      "loss: 0.981731  [ 1952/ 3500]\n",
      "loss: 1.148136  [ 2272/ 3500]\n",
      "loss: 0.878991  [ 2592/ 3500]\n",
      "loss: 0.779687  [ 2912/ 3500]\n",
      "loss: 0.692048  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.888108 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.777995  [   32/ 3500]\n",
      "loss: 0.761653  [  352/ 3500]\n",
      "loss: 0.925943  [  672/ 3500]\n",
      "loss: 0.647601  [  992/ 3500]\n",
      "loss: 0.862332  [ 1312/ 3500]\n",
      "loss: 0.969505  [ 1632/ 3500]\n",
      "loss: 0.644386  [ 1952/ 3500]\n",
      "loss: 0.722053  [ 2272/ 3500]\n",
      "loss: 0.638016  [ 2592/ 3500]\n",
      "loss: 0.803602  [ 2912/ 3500]\n",
      "loss: 1.008086  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.902703 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.609407  [   32/ 3500]\n",
      "loss: 0.607374  [  352/ 3500]\n",
      "loss: 0.651238  [  672/ 3500]\n",
      "loss: 0.681598  [  992/ 3500]\n",
      "loss: 0.680379  [ 1312/ 3500]\n",
      "loss: 1.222943  [ 1632/ 3500]\n",
      "loss: 0.710263  [ 1952/ 3500]\n",
      "loss: 1.050456  [ 2272/ 3500]\n",
      "loss: 0.717917  [ 2592/ 3500]\n",
      "loss: 1.244909  [ 2912/ 3500]\n",
      "loss: 1.031450  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.823388 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.106010  [   32/ 3500]\n",
      "loss: 0.540014  [  352/ 3500]\n",
      "loss: 0.782732  [  672/ 3500]\n",
      "loss: 0.759975  [  992/ 3500]\n",
      "loss: 0.469692  [ 1312/ 3500]\n",
      "loss: 0.510458  [ 1632/ 3500]\n",
      "loss: 0.703620  [ 1952/ 3500]\n",
      "loss: 0.696688  [ 2272/ 3500]\n",
      "loss: 1.016569  [ 2592/ 3500]\n",
      "loss: 0.720808  [ 2912/ 3500]\n",
      "loss: 0.584657  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.846899 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.073030  [   32/ 3500]\n",
      "loss: 1.028609  [  352/ 3500]\n",
      "loss: 0.765201  [  672/ 3500]\n",
      "loss: 0.620643  [  992/ 3500]\n",
      "loss: 0.963432  [ 1312/ 3500]\n",
      "loss: 0.650975  [ 1632/ 3500]\n",
      "loss: 0.899114  [ 1952/ 3500]\n",
      "loss: 0.797281  [ 2272/ 3500]\n",
      "loss: 0.688664  [ 2592/ 3500]\n",
      "loss: 0.494876  [ 2912/ 3500]\n",
      "loss: 0.623677  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.744804 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.789680  [   32/ 3500]\n",
      "loss: 0.568198  [  352/ 3500]\n",
      "loss: 0.476663  [  672/ 3500]\n",
      "loss: 0.674579  [  992/ 3500]\n",
      "loss: 0.693825  [ 1312/ 3500]\n",
      "loss: 0.742603  [ 1632/ 3500]\n",
      "loss: 0.664833  [ 1952/ 3500]\n",
      "loss: 0.740242  [ 2272/ 3500]\n",
      "loss: 0.548338  [ 2592/ 3500]\n",
      "loss: 0.475981  [ 2912/ 3500]\n",
      "loss: 0.747724  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.796647 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.557587  [   32/ 3500]\n",
      "loss: 0.539586  [  352/ 3500]\n",
      "loss: 0.611611  [  672/ 3500]\n",
      "loss: 0.571783  [  992/ 3500]\n",
      "loss: 0.476289  [ 1312/ 3500]\n",
      "loss: 1.023401  [ 1632/ 3500]\n",
      "loss: 0.502683  [ 1952/ 3500]\n",
      "loss: 0.591007  [ 2272/ 3500]\n",
      "loss: 1.074297  [ 2592/ 3500]\n",
      "loss: 0.787153  [ 2912/ 3500]\n",
      "loss: 0.895174  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.680795 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.596605  [   32/ 3500]\n",
      "loss: 0.944054  [  352/ 3500]\n",
      "loss: 0.524541  [  672/ 3500]\n",
      "loss: 0.387477  [  992/ 3500]\n",
      "loss: 0.787221  [ 1312/ 3500]\n",
      "loss: 0.744239  [ 1632/ 3500]\n",
      "loss: 0.460827  [ 1952/ 3500]\n",
      "loss: 0.503501  [ 2272/ 3500]\n",
      "loss: 0.745238  [ 2592/ 3500]\n",
      "loss: 0.460953  [ 2912/ 3500]\n",
      "loss: 0.539201  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.750339 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.520576  [   32/ 3500]\n",
      "loss: 0.695466  [  352/ 3500]\n",
      "loss: 0.565652  [  672/ 3500]\n",
      "loss: 0.555750  [  992/ 3500]\n",
      "loss: 0.624675  [ 1312/ 3500]\n",
      "loss: 0.579746  [ 1632/ 3500]\n",
      "loss: 0.780100  [ 1952/ 3500]\n",
      "loss: 0.416521  [ 2272/ 3500]\n",
      "loss: 0.507306  [ 2592/ 3500]\n",
      "loss: 0.860484  [ 2912/ 3500]\n",
      "loss: 0.547442  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.694484 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.698836  [   32/ 3500]\n",
      "loss: 0.592704  [  352/ 3500]\n",
      "loss: 0.387962  [  672/ 3500]\n",
      "loss: 0.676559  [  992/ 3500]\n",
      "loss: 0.423496  [ 1312/ 3500]\n",
      "loss: 0.579593  [ 1632/ 3500]\n",
      "loss: 0.459910  [ 1952/ 3500]\n",
      "loss: 0.639700  [ 2272/ 3500]\n",
      "loss: 0.397515  [ 2592/ 3500]\n",
      "loss: 0.509929  [ 2912/ 3500]\n",
      "loss: 0.593550  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.868225 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.650059  [   32/ 3500]\n",
      "loss: 0.424505  [  352/ 3500]\n",
      "loss: 0.595686  [  672/ 3500]\n",
      "loss: 0.430597  [  992/ 3500]\n",
      "loss: 0.637036  [ 1312/ 3500]\n",
      "loss: 0.572128  [ 1632/ 3500]\n",
      "loss: 0.612150  [ 1952/ 3500]\n",
      "loss: 0.435558  [ 2272/ 3500]\n",
      "loss: 0.854623  [ 2592/ 3500]\n",
      "loss: 0.557326  [ 2912/ 3500]\n",
      "loss: 0.778463  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.635155 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.639346  [   32/ 3500]\n",
      "loss: 0.376707  [  352/ 3500]\n",
      "loss: 0.779582  [  672/ 3500]\n",
      "loss: 0.322538  [  992/ 3500]\n",
      "loss: 0.688133  [ 1312/ 3500]\n",
      "loss: 0.376319  [ 1632/ 3500]\n",
      "loss: 0.578966  [ 1952/ 3500]\n",
      "loss: 0.864689  [ 2272/ 3500]\n",
      "loss: 0.850634  [ 2592/ 3500]\n",
      "loss: 0.624952  [ 2912/ 3500]\n",
      "loss: 0.615093  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.716258 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.499757  [   32/ 3500]\n",
      "loss: 0.734245  [  352/ 3500]\n",
      "loss: 0.494735  [  672/ 3500]\n",
      "loss: 0.309274  [  992/ 3500]\n",
      "loss: 0.799354  [ 1312/ 3500]\n",
      "loss: 0.370860  [ 1632/ 3500]\n",
      "loss: 0.571070  [ 1952/ 3500]\n",
      "loss: 0.525305  [ 2272/ 3500]\n",
      "loss: 0.665328  [ 2592/ 3500]\n",
      "loss: 0.429605  [ 2912/ 3500]\n",
      "loss: 0.691993  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.636599 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.711114  [   32/ 3500]\n",
      "loss: 0.477573  [  352/ 3500]\n",
      "loss: 0.462146  [  672/ 3500]\n",
      "loss: 0.418021  [  992/ 3500]\n",
      "loss: 0.517829  [ 1312/ 3500]\n",
      "loss: 0.533569  [ 1632/ 3500]\n",
      "loss: 0.407650  [ 1952/ 3500]\n",
      "loss: 0.445195  [ 2272/ 3500]\n",
      "loss: 0.601029  [ 2592/ 3500]\n",
      "loss: 0.460571  [ 2912/ 3500]\n",
      "loss: 0.345410  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.681552 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.406780  [   32/ 3500]\n",
      "loss: 0.530433  [  352/ 3500]\n",
      "loss: 0.306063  [  672/ 3500]\n",
      "loss: 0.479395  [  992/ 3500]\n",
      "loss: 0.425876  [ 1312/ 3500]\n",
      "loss: 0.402902  [ 1632/ 3500]\n",
      "loss: 0.457333  [ 1952/ 3500]\n",
      "loss: 0.358634  [ 2272/ 3500]\n",
      "loss: 0.531231  [ 2592/ 3500]\n",
      "loss: 0.397867  [ 2912/ 3500]\n",
      "loss: 0.544014  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.699729 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.592329  [   32/ 3500]\n",
      "loss: 0.424148  [  352/ 3500]\n",
      "loss: 0.419315  [  672/ 3500]\n",
      "loss: 0.576732  [  992/ 3500]\n",
      "loss: 0.562204  [ 1312/ 3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.423540  [ 1632/ 3500]\n",
      "loss: 0.484495  [ 1952/ 3500]\n",
      "loss: 0.723564  [ 2272/ 3500]\n",
      "loss: 0.516666  [ 2592/ 3500]\n",
      "loss: 0.451332  [ 2912/ 3500]\n",
      "loss: 0.472358  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.637132 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.582623  [   32/ 3500]\n",
      "loss: 0.456807  [  352/ 3500]\n",
      "loss: 0.487610  [  672/ 3500]\n",
      "loss: 0.260963  [  992/ 3500]\n",
      "loss: 0.378184  [ 1312/ 3500]\n",
      "loss: 0.471292  [ 1632/ 3500]\n",
      "loss: 0.404882  [ 1952/ 3500]\n",
      "loss: 0.314655  [ 2272/ 3500]\n",
      "loss: 0.349847  [ 2592/ 3500]\n",
      "loss: 0.301680  [ 2912/ 3500]\n",
      "loss: 0.390162  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.644902 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.416495  [   32/ 3500]\n",
      "loss: 0.330803  [  352/ 3500]\n",
      "loss: 0.554316  [  672/ 3500]\n",
      "loss: 0.513683  [  992/ 3500]\n",
      "loss: 0.640789  [ 1312/ 3500]\n",
      "loss: 0.334063  [ 1632/ 3500]\n",
      "loss: 0.359026  [ 1952/ 3500]\n",
      "loss: 0.372523  [ 2272/ 3500]\n",
      "loss: 0.486634  [ 2592/ 3500]\n",
      "loss: 0.402050  [ 2912/ 3500]\n",
      "loss: 0.355917  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.645025 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.350075  [   32/ 3500]\n",
      "loss: 0.361912  [  352/ 3500]\n",
      "loss: 0.457266  [  672/ 3500]\n",
      "loss: 0.335955  [  992/ 3500]\n",
      "loss: 0.734417  [ 1312/ 3500]\n",
      "loss: 0.398322  [ 1632/ 3500]\n",
      "loss: 0.494592  [ 1952/ 3500]\n",
      "loss: 0.326151  [ 2272/ 3500]\n",
      "loss: 0.485377  [ 2592/ 3500]\n",
      "loss: 0.382855  [ 2912/ 3500]\n",
      "loss: 0.603734  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.710427 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.314126  [   32/ 3500]\n",
      "loss: 0.541321  [  352/ 3500]\n",
      "loss: 0.284515  [  672/ 3500]\n",
      "loss: 0.388501  [  992/ 3500]\n",
      "loss: 0.356050  [ 1312/ 3500]\n",
      "loss: 0.466862  [ 1632/ 3500]\n",
      "loss: 0.304479  [ 1952/ 3500]\n",
      "loss: 0.347919  [ 2272/ 3500]\n",
      "loss: 0.403621  [ 2592/ 3500]\n",
      "loss: 0.310714  [ 2912/ 3500]\n",
      "loss: 0.357198  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.656876 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.319257  [   32/ 3500]\n",
      "loss: 0.594000  [  352/ 3500]\n",
      "loss: 0.443165  [  672/ 3500]\n",
      "loss: 0.401564  [  992/ 3500]\n",
      "loss: 0.466472  [ 1312/ 3500]\n",
      "loss: 0.406450  [ 1632/ 3500]\n",
      "loss: 0.658265  [ 1952/ 3500]\n",
      "loss: 0.339157  [ 2272/ 3500]\n",
      "loss: 0.391247  [ 2592/ 3500]\n",
      "loss: 0.650123  [ 2912/ 3500]\n",
      "loss: 0.381187  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.715269 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.234589  [   32/ 3500]\n",
      "loss: 0.430533  [  352/ 3500]\n",
      "loss: 0.481191  [  672/ 3500]\n",
      "loss: 0.254642  [  992/ 3500]\n",
      "loss: 0.547518  [ 1312/ 3500]\n",
      "loss: 0.445273  [ 1632/ 3500]\n",
      "loss: 0.826367  [ 1952/ 3500]\n",
      "loss: 0.666780  [ 2272/ 3500]\n",
      "loss: 0.535050  [ 2592/ 3500]\n",
      "loss: 0.474807  [ 2912/ 3500]\n",
      "loss: 0.533469  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.658582 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.387016  [   32/ 3500]\n",
      "loss: 0.570499  [  352/ 3500]\n",
      "loss: 0.369627  [  672/ 3500]\n",
      "loss: 0.603848  [  992/ 3500]\n",
      "loss: 0.374497  [ 1312/ 3500]\n",
      "loss: 0.483887  [ 1632/ 3500]\n",
      "loss: 0.381034  [ 1952/ 3500]\n",
      "loss: 0.461020  [ 2272/ 3500]\n",
      "loss: 0.541131  [ 2592/ 3500]\n",
      "loss: 0.493782  [ 2912/ 3500]\n",
      "loss: 0.304927  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.638392 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.351788  [   32/ 3500]\n",
      "loss: 0.359318  [  352/ 3500]\n",
      "loss: 0.418408  [  672/ 3500]\n",
      "loss: 0.253117  [  992/ 3500]\n",
      "loss: 0.347737  [ 1312/ 3500]\n",
      "loss: 0.512481  [ 1632/ 3500]\n",
      "loss: 0.386480  [ 1952/ 3500]\n",
      "loss: 0.481600  [ 2272/ 3500]\n",
      "loss: 0.369721  [ 2592/ 3500]\n",
      "loss: 0.558547  [ 2912/ 3500]\n",
      "loss: 0.309890  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.661119 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.383980  [   32/ 3500]\n",
      "loss: 0.400241  [  352/ 3500]\n",
      "loss: 0.274758  [  672/ 3500]\n",
      "loss: 0.456961  [  992/ 3500]\n",
      "loss: 0.344890  [ 1312/ 3500]\n",
      "loss: 0.401866  [ 1632/ 3500]\n",
      "loss: 0.276028  [ 1952/ 3500]\n",
      "loss: 0.349909  [ 2272/ 3500]\n",
      "loss: 0.377406  [ 2592/ 3500]\n",
      "loss: 0.251325  [ 2912/ 3500]\n",
      "loss: 0.459494  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.713718 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.324488  [   32/ 3500]\n",
      "loss: 0.414245  [  352/ 3500]\n",
      "loss: 0.302047  [  672/ 3500]\n",
      "loss: 0.442086  [  992/ 3500]\n",
      "loss: 0.328706  [ 1312/ 3500]\n",
      "loss: 0.261065  [ 1632/ 3500]\n",
      "loss: 0.515975  [ 1952/ 3500]\n",
      "loss: 0.334232  [ 2272/ 3500]\n",
      "loss: 0.335478  [ 2592/ 3500]\n",
      "loss: 0.332546  [ 2912/ 3500]\n",
      "loss: 0.366401  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.729888 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.441534  [   32/ 3500]\n",
      "loss: 0.294198  [  352/ 3500]\n",
      "loss: 0.383012  [  672/ 3500]\n",
      "loss: 0.550297  [  992/ 3500]\n",
      "loss: 0.269198  [ 1312/ 3500]\n",
      "loss: 0.491305  [ 1632/ 3500]\n",
      "loss: 0.317360  [ 1952/ 3500]\n",
      "loss: 0.357613  [ 2272/ 3500]\n",
      "loss: 0.350316  [ 2592/ 3500]\n",
      "loss: 0.352641  [ 2912/ 3500]\n",
      "loss: 0.230604  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.608494 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.211473  [   32/ 3500]\n",
      "loss: 0.283767  [  352/ 3500]\n",
      "loss: 0.330850  [  672/ 3500]\n",
      "loss: 0.269140  [  992/ 3500]\n",
      "loss: 0.478662  [ 1312/ 3500]\n",
      "loss: 0.374186  [ 1632/ 3500]\n",
      "loss: 0.585889  [ 1952/ 3500]\n",
      "loss: 0.420400  [ 2272/ 3500]\n",
      "loss: 0.428676  [ 2592/ 3500]\n",
      "loss: 0.421102  [ 2912/ 3500]\n",
      "loss: 0.357255  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.712873 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.668632  [   32/ 3500]\n",
      "loss: 0.224090  [  352/ 3500]\n",
      "loss: 0.196258  [  672/ 3500]\n",
      "loss: 0.312861  [  992/ 3500]\n",
      "loss: 0.523814  [ 1312/ 3500]\n",
      "loss: 0.455134  [ 1632/ 3500]\n",
      "loss: 0.369819  [ 1952/ 3500]\n",
      "loss: 0.606607  [ 2272/ 3500]\n",
      "loss: 0.267211  [ 2592/ 3500]\n",
      "loss: 0.454831  [ 2912/ 3500]\n",
      "loss: 0.265361  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.013027 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.994030  [   32/ 3500]\n",
      "loss: 0.246410  [  352/ 3500]\n",
      "loss: 0.317107  [  672/ 3500]\n",
      "loss: 0.529356  [  992/ 3500]\n",
      "loss: 0.353611  [ 1312/ 3500]\n",
      "loss: 0.236757  [ 1632/ 3500]\n",
      "loss: 0.370059  [ 1952/ 3500]\n",
      "loss: 0.225548  [ 2272/ 3500]\n",
      "loss: 0.327680  [ 2592/ 3500]\n",
      "loss: 0.358590  [ 2912/ 3500]\n",
      "loss: 0.384696  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.671846 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.295541  [   32/ 3500]\n",
      "loss: 0.367473  [  352/ 3500]\n",
      "loss: 0.346029  [  672/ 3500]\n",
      "loss: 0.368340  [  992/ 3500]\n",
      "loss: 0.483445  [ 1312/ 3500]\n",
      "loss: 0.236162  [ 1632/ 3500]\n",
      "loss: 0.469328  [ 1952/ 3500]\n",
      "loss: 0.466634  [ 2272/ 3500]\n",
      "loss: 0.338553  [ 2592/ 3500]\n",
      "loss: 0.376560  [ 2912/ 3500]\n",
      "loss: 0.329258  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.707120 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.754037  [   32/ 3500]\n",
      "loss: 0.347819  [  352/ 3500]\n",
      "loss: 0.185582  [  672/ 3500]\n",
      "loss: 0.596486  [  992/ 3500]\n",
      "loss: 0.449330  [ 1312/ 3500]\n",
      "loss: 0.455897  [ 1632/ 3500]\n",
      "loss: 0.332374  [ 1952/ 3500]\n",
      "loss: 0.395510  [ 2272/ 3500]\n",
      "loss: 0.362084  [ 2592/ 3500]\n",
      "loss: 0.266977  [ 2912/ 3500]\n",
      "loss: 0.120915  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.596719 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.300092  [   32/ 3500]\n",
      "loss: 0.514157  [  352/ 3500]\n",
      "loss: 0.172946  [  672/ 3500]\n",
      "loss: 0.258114  [  992/ 3500]\n",
      "loss: 0.353878  [ 1312/ 3500]\n",
      "loss: 0.183345  [ 1632/ 3500]\n",
      "loss: 0.207869  [ 1952/ 3500]\n",
      "loss: 0.375652  [ 2272/ 3500]\n",
      "loss: 0.406295  [ 2592/ 3500]\n",
      "loss: 0.374845  [ 2912/ 3500]\n",
      "loss: 0.324465  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.632916 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.248077  [   32/ 3500]\n",
      "loss: 0.287134  [  352/ 3500]\n",
      "loss: 0.299317  [  672/ 3500]\n",
      "loss: 0.455724  [  992/ 3500]\n",
      "loss: 0.208517  [ 1312/ 3500]\n",
      "loss: 0.266896  [ 1632/ 3500]\n",
      "loss: 0.363648  [ 1952/ 3500]\n",
      "loss: 0.266258  [ 2272/ 3500]\n",
      "loss: 0.355486  [ 2592/ 3500]\n",
      "loss: 0.282001  [ 2912/ 3500]\n",
      "loss: 0.311488  [ 3232/ 3500]\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.599509 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.423517  [   32/ 3500]\n",
      "loss: 0.242327  [  352/ 3500]\n",
      "loss: 0.415949  [  672/ 3500]\n",
      "loss: 0.259434  [  992/ 3500]\n",
      "loss: 0.398284  [ 1312/ 3500]\n",
      "loss: 0.337752  [ 1632/ 3500]\n",
      "loss: 0.377939  [ 1952/ 3500]\n",
      "loss: 0.258034  [ 2272/ 3500]\n",
      "loss: 0.343807  [ 2592/ 3500]\n",
      "loss: 0.338138  [ 2912/ 3500]\n",
      "loss: 0.342958  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.709603 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.345589  [   32/ 3500]\n",
      "loss: 0.388362  [  352/ 3500]\n",
      "loss: 0.449071  [  672/ 3500]\n",
      "loss: 0.418597  [  992/ 3500]\n",
      "loss: 0.308382  [ 1312/ 3500]\n",
      "loss: 0.433924  [ 1632/ 3500]\n",
      "loss: 0.244326  [ 1952/ 3500]\n",
      "loss: 0.334320  [ 2272/ 3500]\n",
      "loss: 0.353452  [ 2592/ 3500]\n",
      "loss: 0.277622  [ 2912/ 3500]\n",
      "loss: 0.303564  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.607376 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.322914  [   32/ 3500]\n",
      "loss: 0.257406  [  352/ 3500]\n",
      "loss: 0.357287  [  672/ 3500]\n",
      "loss: 0.324536  [  992/ 3500]\n",
      "loss: 0.179609  [ 1312/ 3500]\n",
      "loss: 0.258377  [ 1632/ 3500]\n",
      "loss: 0.340431  [ 1952/ 3500]\n",
      "loss: 0.283265  [ 2272/ 3500]\n",
      "loss: 0.269228  [ 2592/ 3500]\n",
      "loss: 0.186895  [ 2912/ 3500]\n",
      "loss: 0.299118  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.923155 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.674244  [   32/ 3500]\n",
      "loss: 0.305712  [  352/ 3500]\n",
      "loss: 0.361863  [  672/ 3500]\n",
      "loss: 0.368168  [  992/ 3500]\n",
      "loss: 0.365511  [ 1312/ 3500]\n",
      "loss: 0.340873  [ 1632/ 3500]\n",
      "loss: 0.269234  [ 1952/ 3500]\n",
      "loss: 0.421942  [ 2272/ 3500]\n",
      "loss: 0.267327  [ 2592/ 3500]\n",
      "loss: 0.265164  [ 2912/ 3500]\n",
      "loss: 0.305033  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.650666 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.289620  [   32/ 3500]\n",
      "loss: 0.205869  [  352/ 3500]\n",
      "loss: 0.338940  [  672/ 3500]\n",
      "loss: 0.234355  [  992/ 3500]\n",
      "loss: 0.317027  [ 1312/ 3500]\n",
      "loss: 0.136262  [ 1632/ 3500]\n",
      "loss: 0.479896  [ 1952/ 3500]\n",
      "loss: 0.364832  [ 2272/ 3500]\n",
      "loss: 0.295705  [ 2592/ 3500]\n",
      "loss: 0.218589  [ 2912/ 3500]\n",
      "loss: 0.168929  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.580278 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.180260  [   32/ 3500]\n",
      "loss: 0.242045  [  352/ 3500]\n",
      "loss: 0.361377  [  672/ 3500]\n",
      "loss: 0.221055  [  992/ 3500]\n",
      "loss: 0.284020  [ 1312/ 3500]\n",
      "loss: 0.565201  [ 1632/ 3500]\n",
      "loss: 0.384657  [ 1952/ 3500]\n",
      "loss: 0.215709  [ 2272/ 3500]\n",
      "loss: 0.459313  [ 2592/ 3500]\n",
      "loss: 0.416283  [ 2912/ 3500]\n",
      "loss: 0.329698  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.629172 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.335583  [   32/ 3500]\n",
      "loss: 0.258063  [  352/ 3500]\n",
      "loss: 0.195405  [  672/ 3500]\n",
      "loss: 0.410613  [  992/ 3500]\n",
      "loss: 0.383092  [ 1312/ 3500]\n",
      "loss: 0.335731  [ 1632/ 3500]\n",
      "loss: 0.251336  [ 1952/ 3500]\n",
      "loss: 0.246424  [ 2272/ 3500]\n",
      "loss: 0.198918  [ 2592/ 3500]\n",
      "loss: 0.393140  [ 2912/ 3500]\n",
      "loss: 0.233210  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.689845 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.509061  [   32/ 3500]\n",
      "loss: 0.248770  [  352/ 3500]\n",
      "loss: 0.219262  [  672/ 3500]\n",
      "loss: 0.236622  [  992/ 3500]\n",
      "loss: 0.288080  [ 1312/ 3500]\n",
      "loss: 0.267665  [ 1632/ 3500]\n",
      "loss: 0.195297  [ 1952/ 3500]\n",
      "loss: 0.308609  [ 2272/ 3500]\n",
      "loss: 0.340678  [ 2592/ 3500]\n",
      "loss: 0.337854  [ 2912/ 3500]\n",
      "loss: 0.357072  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.596998 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.273044  [   32/ 3500]\n",
      "loss: 0.340734  [  352/ 3500]\n",
      "loss: 0.214129  [  672/ 3500]\n",
      "loss: 0.223194  [  992/ 3500]\n",
      "loss: 0.368953  [ 1312/ 3500]\n",
      "loss: 0.231086  [ 1632/ 3500]\n",
      "loss: 0.157472  [ 1952/ 3500]\n",
      "loss: 0.211097  [ 2272/ 3500]\n",
      "loss: 0.316932  [ 2592/ 3500]\n",
      "loss: 0.406195  [ 2912/ 3500]\n",
      "loss: 0.162665  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.609032 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.308340  [   32/ 3500]\n",
      "loss: 0.249453  [  352/ 3500]\n",
      "loss: 0.385237  [  672/ 3500]\n",
      "loss: 0.302474  [  992/ 3500]\n",
      "loss: 0.187717  [ 1312/ 3500]\n",
      "loss: 0.189817  [ 1632/ 3500]\n",
      "loss: 0.228281  [ 1952/ 3500]\n",
      "loss: 0.353460  [ 2272/ 3500]\n",
      "loss: 0.188637  [ 2592/ 3500]\n",
      "loss: 0.220389  [ 2912/ 3500]\n",
      "loss: 0.159183  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.629876 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.352737  [   32/ 3500]\n",
      "loss: 0.392154  [  352/ 3500]\n",
      "loss: 0.171543  [  672/ 3500]\n",
      "loss: 0.145854  [  992/ 3500]\n",
      "loss: 0.355018  [ 1312/ 3500]\n",
      "loss: 0.198472  [ 1632/ 3500]\n",
      "loss: 0.259360  [ 1952/ 3500]\n",
      "loss: 0.254069  [ 2272/ 3500]\n",
      "loss: 0.328016  [ 2592/ 3500]\n",
      "loss: 0.170232  [ 2912/ 3500]\n",
      "loss: 0.255196  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.598622 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.184137  [   32/ 3500]\n",
      "loss: 0.289780  [  352/ 3500]\n",
      "loss: 0.221617  [  672/ 3500]\n",
      "loss: 0.210189  [  992/ 3500]\n",
      "loss: 0.242858  [ 1312/ 3500]\n",
      "loss: 0.206450  [ 1632/ 3500]\n",
      "loss: 0.292221  [ 1952/ 3500]\n",
      "loss: 0.186004  [ 2272/ 3500]\n",
      "loss: 0.162382  [ 2592/ 3500]\n",
      "loss: 0.257666  [ 2912/ 3500]\n",
      "loss: 0.304787  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.577290 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.199721  [   32/ 3500]\n",
      "loss: 0.217447  [  352/ 3500]\n",
      "loss: 0.267454  [  672/ 3500]\n",
      "loss: 0.326232  [  992/ 3500]\n",
      "loss: 0.231767  [ 1312/ 3500]\n",
      "loss: 0.204243  [ 1632/ 3500]\n",
      "loss: 0.272585  [ 1952/ 3500]\n",
      "loss: 0.383485  [ 2272/ 3500]\n",
      "loss: 0.299609  [ 2592/ 3500]\n",
      "loss: 0.273759  [ 2912/ 3500]\n",
      "loss: 0.469249  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.612529 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Epochs = 50, Learning Rate = 1e-5\n",
    "model_50_1e_5 = NeuralNetwork_Conv().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_50_1e_5.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 50\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_5, loss_fn, optimizer))\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_5, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this model to CNN_50_1e_5.pth\n",
    "torch.save(model_50_1e_5.state_dict(), \"./CNN_50_1e_5.pth\")\n",
    "#files.download(\"/content/gdrive/MyDrive/Colab Notebooks/CompVis/CNN_50_1e_5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1648860669536,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "dRzJE74qOQL1",
    "outputId": "2c0b9940-252e-49b9-c265-1311ec396715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 2.362893  [   32/ 3500]\n",
      "loss: 1.719367  [  352/ 3500]\n",
      "loss: 1.728174  [  672/ 3500]\n",
      "loss: 1.694170  [  992/ 3500]\n",
      "loss: 1.536605  [ 1312/ 3500]\n",
      "loss: 1.575056  [ 1632/ 3500]\n",
      "loss: 1.492616  [ 1952/ 3500]\n",
      "loss: 1.499861  [ 2272/ 3500]\n",
      "loss: 1.467837  [ 2592/ 3500]\n",
      "loss: 1.518072  [ 2912/ 3500]\n",
      "loss: 1.528755  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 36.5%, Avg loss: 1.480090 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.411827  [   32/ 3500]\n",
      "loss: 1.732156  [  352/ 3500]\n",
      "loss: 1.487048  [  672/ 3500]\n",
      "loss: 1.429009  [  992/ 3500]\n",
      "loss: 1.580583  [ 1312/ 3500]\n",
      "loss: 1.366726  [ 1632/ 3500]\n",
      "loss: 1.514494  [ 1952/ 3500]\n",
      "loss: 1.602366  [ 2272/ 3500]\n",
      "loss: 1.341928  [ 2592/ 3500]\n",
      "loss: 1.431775  [ 2912/ 3500]\n",
      "loss: 1.504369  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.372910 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.271585  [   32/ 3500]\n",
      "loss: 1.424435  [  352/ 3500]\n",
      "loss: 1.381410  [  672/ 3500]\n",
      "loss: 1.324260  [  992/ 3500]\n",
      "loss: 1.350163  [ 1312/ 3500]\n",
      "loss: 1.444792  [ 1632/ 3500]\n",
      "loss: 1.317471  [ 1952/ 3500]\n",
      "loss: 1.277967  [ 2272/ 3500]\n",
      "loss: 1.366591  [ 2592/ 3500]\n",
      "loss: 1.652879  [ 2912/ 3500]\n",
      "loss: 1.273192  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.317679 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.256290  [   32/ 3500]\n",
      "loss: 1.388019  [  352/ 3500]\n",
      "loss: 1.182477  [  672/ 3500]\n",
      "loss: 1.273518  [  992/ 3500]\n",
      "loss: 1.184842  [ 1312/ 3500]\n",
      "loss: 1.321657  [ 1632/ 3500]\n",
      "loss: 1.195785  [ 1952/ 3500]\n",
      "loss: 1.375158  [ 2272/ 3500]\n",
      "loss: 1.384967  [ 2592/ 3500]\n",
      "loss: 1.407794  [ 2912/ 3500]\n",
      "loss: 1.430448  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 1.261989 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.257155  [   32/ 3500]\n",
      "loss: 1.409414  [  352/ 3500]\n",
      "loss: 1.287140  [  672/ 3500]\n",
      "loss: 1.586627  [  992/ 3500]\n",
      "loss: 1.225730  [ 1312/ 3500]\n",
      "loss: 1.345537  [ 1632/ 3500]\n",
      "loss: 1.212914  [ 1952/ 3500]\n",
      "loss: 1.326095  [ 2272/ 3500]\n",
      "loss: 1.158784  [ 2592/ 3500]\n",
      "loss: 1.168179  [ 2912/ 3500]\n",
      "loss: 1.174169  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.240733 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.172209  [   32/ 3500]\n",
      "loss: 1.229393  [  352/ 3500]\n",
      "loss: 1.258170  [  672/ 3500]\n",
      "loss: 1.210584  [  992/ 3500]\n",
      "loss: 1.228743  [ 1312/ 3500]\n",
      "loss: 1.269657  [ 1632/ 3500]\n",
      "loss: 1.193772  [ 1952/ 3500]\n",
      "loss: 1.210671  [ 2272/ 3500]\n",
      "loss: 1.273006  [ 2592/ 3500]\n",
      "loss: 1.181378  [ 2912/ 3500]\n",
      "loss: 1.171329  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.203520 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.141459  [   32/ 3500]\n",
      "loss: 1.218844  [  352/ 3500]\n",
      "loss: 1.096367  [  672/ 3500]\n",
      "loss: 1.254581  [  992/ 3500]\n",
      "loss: 1.112627  [ 1312/ 3500]\n",
      "loss: 1.200253  [ 1632/ 3500]\n",
      "loss: 1.419283  [ 1952/ 3500]\n",
      "loss: 1.224438  [ 2272/ 3500]\n",
      "loss: 1.272005  [ 2592/ 3500]\n",
      "loss: 1.034671  [ 2912/ 3500]\n",
      "loss: 1.300331  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.171410 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.183530  [   32/ 3500]\n",
      "loss: 1.351847  [  352/ 3500]\n",
      "loss: 1.427025  [  672/ 3500]\n",
      "loss: 1.207769  [  992/ 3500]\n",
      "loss: 1.317546  [ 1312/ 3500]\n",
      "loss: 0.958309  [ 1632/ 3500]\n",
      "loss: 1.162024  [ 1952/ 3500]\n",
      "loss: 1.203968  [ 2272/ 3500]\n",
      "loss: 1.095701  [ 2592/ 3500]\n",
      "loss: 1.155356  [ 2912/ 3500]\n",
      "loss: 1.041423  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.144654 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.031439  [   32/ 3500]\n",
      "loss: 1.276712  [  352/ 3500]\n",
      "loss: 1.295488  [  672/ 3500]\n",
      "loss: 1.273855  [  992/ 3500]\n",
      "loss: 1.368225  [ 1312/ 3500]\n",
      "loss: 1.135120  [ 1632/ 3500]\n",
      "loss: 1.213959  [ 1952/ 3500]\n",
      "loss: 1.171000  [ 2272/ 3500]\n",
      "loss: 1.069853  [ 2592/ 3500]\n",
      "loss: 1.143436  [ 2912/ 3500]\n",
      "loss: 1.034897  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.136147 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.535116  [   32/ 3500]\n",
      "loss: 1.293786  [  352/ 3500]\n",
      "loss: 1.221900  [  672/ 3500]\n",
      "loss: 1.150686  [  992/ 3500]\n",
      "loss: 1.042733  [ 1312/ 3500]\n",
      "loss: 1.249987  [ 1632/ 3500]\n",
      "loss: 1.130343  [ 1952/ 3500]\n",
      "loss: 1.143902  [ 2272/ 3500]\n",
      "loss: 1.334418  [ 2592/ 3500]\n",
      "loss: 1.163600  [ 2912/ 3500]\n",
      "loss: 0.907527  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.105006 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.995812  [   32/ 3500]\n",
      "loss: 1.005576  [  352/ 3500]\n",
      "loss: 1.133362  [  672/ 3500]\n",
      "loss: 1.050910  [  992/ 3500]\n",
      "loss: 1.023254  [ 1312/ 3500]\n",
      "loss: 0.999915  [ 1632/ 3500]\n",
      "loss: 1.308114  [ 1952/ 3500]\n",
      "loss: 1.044438  [ 2272/ 3500]\n",
      "loss: 1.093521  [ 2592/ 3500]\n",
      "loss: 0.946954  [ 2912/ 3500]\n",
      "loss: 1.045393  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.089005 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.171290  [   32/ 3500]\n",
      "loss: 1.289499  [  352/ 3500]\n",
      "loss: 1.010535  [  672/ 3500]\n",
      "loss: 1.120080  [  992/ 3500]\n",
      "loss: 0.982679  [ 1312/ 3500]\n",
      "loss: 1.089956  [ 1632/ 3500]\n",
      "loss: 1.269943  [ 1952/ 3500]\n",
      "loss: 1.153085  [ 2272/ 3500]\n",
      "loss: 0.913748  [ 2592/ 3500]\n",
      "loss: 1.046010  [ 2912/ 3500]\n",
      "loss: 1.046651  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.068902 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.026857  [   32/ 3500]\n",
      "loss: 1.212583  [  352/ 3500]\n",
      "loss: 1.038859  [  672/ 3500]\n",
      "loss: 1.098616  [  992/ 3500]\n",
      "loss: 1.078707  [ 1312/ 3500]\n",
      "loss: 0.985019  [ 1632/ 3500]\n",
      "loss: 1.169871  [ 1952/ 3500]\n",
      "loss: 1.031306  [ 2272/ 3500]\n",
      "loss: 0.921307  [ 2592/ 3500]\n",
      "loss: 1.265093  [ 2912/ 3500]\n",
      "loss: 1.131085  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.075062 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.995135  [   32/ 3500]\n",
      "loss: 1.236267  [  352/ 3500]\n",
      "loss: 0.812339  [  672/ 3500]\n",
      "loss: 1.132196  [  992/ 3500]\n",
      "loss: 0.963086  [ 1312/ 3500]\n",
      "loss: 0.952881  [ 1632/ 3500]\n",
      "loss: 1.211537  [ 1952/ 3500]\n",
      "loss: 1.096803  [ 2272/ 3500]\n",
      "loss: 1.257031  [ 2592/ 3500]\n",
      "loss: 0.991562  [ 2912/ 3500]\n",
      "loss: 1.210181  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.044114 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.023083  [   32/ 3500]\n",
      "loss: 0.835596  [  352/ 3500]\n",
      "loss: 0.922823  [  672/ 3500]\n",
      "loss: 1.084162  [  992/ 3500]\n",
      "loss: 0.894520  [ 1312/ 3500]\n",
      "loss: 1.012130  [ 1632/ 3500]\n",
      "loss: 0.960948  [ 1952/ 3500]\n",
      "loss: 1.091774  [ 2272/ 3500]\n",
      "loss: 1.011765  [ 2592/ 3500]\n",
      "loss: 0.909656  [ 2912/ 3500]\n",
      "loss: 0.941689  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.050164 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.845117  [   32/ 3500]\n",
      "loss: 0.995916  [  352/ 3500]\n",
      "loss: 0.846926  [  672/ 3500]\n",
      "loss: 0.887405  [  992/ 3500]\n",
      "loss: 0.953373  [ 1312/ 3500]\n",
      "loss: 1.274537  [ 1632/ 3500]\n",
      "loss: 0.990079  [ 1952/ 3500]\n",
      "loss: 1.074173  [ 2272/ 3500]\n",
      "loss: 1.054218  [ 2592/ 3500]\n",
      "loss: 1.012491  [ 2912/ 3500]\n",
      "loss: 0.978805  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.020235 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.169469  [   32/ 3500]\n",
      "loss: 0.895425  [  352/ 3500]\n",
      "loss: 1.026494  [  672/ 3500]\n",
      "loss: 0.889809  [  992/ 3500]\n",
      "loss: 1.019176  [ 1312/ 3500]\n",
      "loss: 0.876689  [ 1632/ 3500]\n",
      "loss: 0.935804  [ 1952/ 3500]\n",
      "loss: 0.992825  [ 2272/ 3500]\n",
      "loss: 1.006958  [ 2592/ 3500]\n",
      "loss: 1.065873  [ 2912/ 3500]\n",
      "loss: 0.870054  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.010956 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.964059  [   32/ 3500]\n",
      "loss: 0.877216  [  352/ 3500]\n",
      "loss: 1.068205  [  672/ 3500]\n",
      "loss: 0.921030  [  992/ 3500]\n",
      "loss: 1.117265  [ 1312/ 3500]\n",
      "loss: 1.253287  [ 1632/ 3500]\n",
      "loss: 0.835155  [ 1952/ 3500]\n",
      "loss: 1.074427  [ 2272/ 3500]\n",
      "loss: 1.030118  [ 2592/ 3500]\n",
      "loss: 1.022765  [ 2912/ 3500]\n",
      "loss: 1.019638  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.000567 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.873371  [   32/ 3500]\n",
      "loss: 0.839611  [  352/ 3500]\n",
      "loss: 0.781039  [  672/ 3500]\n",
      "loss: 0.943079  [  992/ 3500]\n",
      "loss: 0.879612  [ 1312/ 3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.052608  [ 1632/ 3500]\n",
      "loss: 1.025718  [ 1952/ 3500]\n",
      "loss: 1.149382  [ 2272/ 3500]\n",
      "loss: 0.664246  [ 2592/ 3500]\n",
      "loss: 0.914090  [ 2912/ 3500]\n",
      "loss: 1.283971  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.995457 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.884706  [   32/ 3500]\n",
      "loss: 0.805306  [  352/ 3500]\n",
      "loss: 0.724484  [  672/ 3500]\n",
      "loss: 0.945628  [  992/ 3500]\n",
      "loss: 0.949989  [ 1312/ 3500]\n",
      "loss: 0.940958  [ 1632/ 3500]\n",
      "loss: 1.082742  [ 1952/ 3500]\n",
      "loss: 0.941980  [ 2272/ 3500]\n",
      "loss: 1.106454  [ 2592/ 3500]\n",
      "loss: 0.930109  [ 2912/ 3500]\n",
      "loss: 0.911195  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.977270 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.821337  [   32/ 3500]\n",
      "loss: 1.143401  [  352/ 3500]\n",
      "loss: 0.796302  [  672/ 3500]\n",
      "loss: 0.815780  [  992/ 3500]\n",
      "loss: 0.969212  [ 1312/ 3500]\n",
      "loss: 1.035880  [ 1632/ 3500]\n",
      "loss: 0.908402  [ 1952/ 3500]\n",
      "loss: 0.865121  [ 2272/ 3500]\n",
      "loss: 0.887529  [ 2592/ 3500]\n",
      "loss: 0.946052  [ 2912/ 3500]\n",
      "loss: 0.834317  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.970066 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.989521  [   32/ 3500]\n",
      "loss: 0.877264  [  352/ 3500]\n",
      "loss: 0.867989  [  672/ 3500]\n",
      "loss: 1.031836  [  992/ 3500]\n",
      "loss: 0.935415  [ 1312/ 3500]\n",
      "loss: 0.804756  [ 1632/ 3500]\n",
      "loss: 0.840906  [ 1952/ 3500]\n",
      "loss: 0.913176  [ 2272/ 3500]\n",
      "loss: 0.922252  [ 2592/ 3500]\n",
      "loss: 1.014303  [ 2912/ 3500]\n",
      "loss: 1.189963  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.959981 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.713609  [   32/ 3500]\n",
      "loss: 0.955881  [  352/ 3500]\n",
      "loss: 0.966119  [  672/ 3500]\n",
      "loss: 1.027041  [  992/ 3500]\n",
      "loss: 1.067416  [ 1312/ 3500]\n",
      "loss: 0.887124  [ 1632/ 3500]\n",
      "loss: 0.922270  [ 1952/ 3500]\n",
      "loss: 0.782295  [ 2272/ 3500]\n",
      "loss: 0.980177  [ 2592/ 3500]\n",
      "loss: 1.007422  [ 2912/ 3500]\n",
      "loss: 0.822570  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.953757 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.725960  [   32/ 3500]\n",
      "loss: 0.896635  [  352/ 3500]\n",
      "loss: 0.942938  [  672/ 3500]\n",
      "loss: 1.044108  [  992/ 3500]\n",
      "loss: 0.987214  [ 1312/ 3500]\n",
      "loss: 0.799604  [ 1632/ 3500]\n",
      "loss: 0.514152  [ 1952/ 3500]\n",
      "loss: 0.892040  [ 2272/ 3500]\n",
      "loss: 0.904064  [ 2592/ 3500]\n",
      "loss: 0.920013  [ 2912/ 3500]\n",
      "loss: 0.847438  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.945199 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.927190  [   32/ 3500]\n",
      "loss: 1.165818  [  352/ 3500]\n",
      "loss: 0.939399  [  672/ 3500]\n",
      "loss: 0.933397  [  992/ 3500]\n",
      "loss: 0.880088  [ 1312/ 3500]\n",
      "loss: 0.752072  [ 1632/ 3500]\n",
      "loss: 0.921518  [ 1952/ 3500]\n",
      "loss: 0.792408  [ 2272/ 3500]\n",
      "loss: 0.853137  [ 2592/ 3500]\n",
      "loss: 0.868766  [ 2912/ 3500]\n",
      "loss: 0.762689  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.943448 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.084275  [   32/ 3500]\n",
      "loss: 0.758712  [  352/ 3500]\n",
      "loss: 0.884055  [  672/ 3500]\n",
      "loss: 0.880825  [  992/ 3500]\n",
      "loss: 1.225454  [ 1312/ 3500]\n",
      "loss: 0.773657  [ 1632/ 3500]\n",
      "loss: 0.772765  [ 1952/ 3500]\n",
      "loss: 0.997469  [ 2272/ 3500]\n",
      "loss: 0.956305  [ 2592/ 3500]\n",
      "loss: 1.153718  [ 2912/ 3500]\n",
      "loss: 0.814031  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.926693 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.847074  [   32/ 3500]\n",
      "loss: 0.735641  [  352/ 3500]\n",
      "loss: 0.884289  [  672/ 3500]\n",
      "loss: 0.723876  [  992/ 3500]\n",
      "loss: 0.881497  [ 1312/ 3500]\n",
      "loss: 1.175437  [ 1632/ 3500]\n",
      "loss: 0.739754  [ 1952/ 3500]\n",
      "loss: 1.197291  [ 2272/ 3500]\n",
      "loss: 0.827471  [ 2592/ 3500]\n",
      "loss: 0.718584  [ 2912/ 3500]\n",
      "loss: 0.838346  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.934745 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.808680  [   32/ 3500]\n",
      "loss: 0.921058  [  352/ 3500]\n",
      "loss: 0.778080  [  672/ 3500]\n",
      "loss: 0.694192  [  992/ 3500]\n",
      "loss: 0.902927  [ 1312/ 3500]\n",
      "loss: 0.825338  [ 1632/ 3500]\n",
      "loss: 0.976011  [ 1952/ 3500]\n",
      "loss: 0.815052  [ 2272/ 3500]\n",
      "loss: 0.906509  [ 2592/ 3500]\n",
      "loss: 0.802221  [ 2912/ 3500]\n",
      "loss: 0.905765  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.925697 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.156599  [   32/ 3500]\n",
      "loss: 0.907594  [  352/ 3500]\n",
      "loss: 0.954105  [  672/ 3500]\n",
      "loss: 1.094414  [  992/ 3500]\n",
      "loss: 1.142818  [ 1312/ 3500]\n",
      "loss: 0.819819  [ 1632/ 3500]\n",
      "loss: 0.764855  [ 1952/ 3500]\n",
      "loss: 0.856214  [ 2272/ 3500]\n",
      "loss: 0.781458  [ 2592/ 3500]\n",
      "loss: 0.855849  [ 2912/ 3500]\n",
      "loss: 0.899256  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.923765 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.829829  [   32/ 3500]\n",
      "loss: 0.841804  [  352/ 3500]\n",
      "loss: 0.599994  [  672/ 3500]\n",
      "loss: 0.933744  [  992/ 3500]\n",
      "loss: 0.874182  [ 1312/ 3500]\n",
      "loss: 0.845975  [ 1632/ 3500]\n",
      "loss: 0.807789  [ 1952/ 3500]\n",
      "loss: 0.715032  [ 2272/ 3500]\n",
      "loss: 0.915788  [ 2592/ 3500]\n",
      "loss: 0.823452  [ 2912/ 3500]\n",
      "loss: 0.804793  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.908933 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.013452  [   32/ 3500]\n",
      "loss: 0.773033  [  352/ 3500]\n",
      "loss: 0.947773  [  672/ 3500]\n",
      "loss: 1.096209  [  992/ 3500]\n",
      "loss: 0.623466  [ 1312/ 3500]\n",
      "loss: 0.943359  [ 1632/ 3500]\n",
      "loss: 0.772428  [ 1952/ 3500]\n",
      "loss: 1.346557  [ 2272/ 3500]\n",
      "loss: 0.798348  [ 2592/ 3500]\n",
      "loss: 0.988150  [ 2912/ 3500]\n",
      "loss: 0.974839  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.900369 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.927288  [   32/ 3500]\n",
      "loss: 0.990229  [  352/ 3500]\n",
      "loss: 0.788471  [  672/ 3500]\n",
      "loss: 0.731108  [  992/ 3500]\n",
      "loss: 1.072840  [ 1312/ 3500]\n",
      "loss: 1.099030  [ 1632/ 3500]\n",
      "loss: 0.745675  [ 1952/ 3500]\n",
      "loss: 1.089407  [ 2272/ 3500]\n",
      "loss: 0.857913  [ 2592/ 3500]\n",
      "loss: 0.937589  [ 2912/ 3500]\n",
      "loss: 0.786469  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.890460 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.130222  [   32/ 3500]\n",
      "loss: 0.728290  [  352/ 3500]\n",
      "loss: 0.707072  [  672/ 3500]\n",
      "loss: 0.760236  [  992/ 3500]\n",
      "loss: 1.026119  [ 1312/ 3500]\n",
      "loss: 0.730555  [ 1632/ 3500]\n",
      "loss: 0.845666  [ 1952/ 3500]\n",
      "loss: 1.483505  [ 2272/ 3500]\n",
      "loss: 0.852541  [ 2592/ 3500]\n",
      "loss: 0.841620  [ 2912/ 3500]\n",
      "loss: 0.684122  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.882931 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.638854  [   32/ 3500]\n",
      "loss: 0.872286  [  352/ 3500]\n",
      "loss: 0.772504  [  672/ 3500]\n",
      "loss: 0.945258  [  992/ 3500]\n",
      "loss: 0.739860  [ 1312/ 3500]\n",
      "loss: 0.853228  [ 1632/ 3500]\n",
      "loss: 0.967167  [ 1952/ 3500]\n",
      "loss: 0.988693  [ 2272/ 3500]\n",
      "loss: 0.857424  [ 2592/ 3500]\n",
      "loss: 0.920170  [ 2912/ 3500]\n",
      "loss: 0.989999  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.877692 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.802664  [   32/ 3500]\n",
      "loss: 0.745367  [  352/ 3500]\n",
      "loss: 0.873214  [  672/ 3500]\n",
      "loss: 0.965995  [  992/ 3500]\n",
      "loss: 0.819248  [ 1312/ 3500]\n",
      "loss: 0.737781  [ 1632/ 3500]\n",
      "loss: 0.577915  [ 1952/ 3500]\n",
      "loss: 0.842399  [ 2272/ 3500]\n",
      "loss: 0.832881  [ 2592/ 3500]\n",
      "loss: 0.954296  [ 2912/ 3500]\n",
      "loss: 0.829360  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.878091 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.822441  [   32/ 3500]\n",
      "loss: 0.589647  [  352/ 3500]\n",
      "loss: 0.851063  [  672/ 3500]\n",
      "loss: 0.678195  [  992/ 3500]\n",
      "loss: 0.803437  [ 1312/ 3500]\n",
      "loss: 0.738107  [ 1632/ 3500]\n",
      "loss: 0.841038  [ 1952/ 3500]\n",
      "loss: 0.870713  [ 2272/ 3500]\n",
      "loss: 0.721245  [ 2592/ 3500]\n",
      "loss: 0.713245  [ 2912/ 3500]\n",
      "loss: 1.049004  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.878604 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.842594  [   32/ 3500]\n",
      "loss: 0.770567  [  352/ 3500]\n",
      "loss: 0.690843  [  672/ 3500]\n",
      "loss: 0.541267  [  992/ 3500]\n",
      "loss: 0.794572  [ 1312/ 3500]\n",
      "loss: 0.726695  [ 1632/ 3500]\n",
      "loss: 0.991700  [ 1952/ 3500]\n",
      "loss: 0.912547  [ 2272/ 3500]\n",
      "loss: 0.809450  [ 2592/ 3500]\n",
      "loss: 0.981267  [ 2912/ 3500]\n",
      "loss: 0.795308  [ 3232/ 3500]\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.873463 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.833951  [   32/ 3500]\n",
      "loss: 0.903354  [  352/ 3500]\n",
      "loss: 0.843526  [  672/ 3500]\n",
      "loss: 0.971928  [  992/ 3500]\n",
      "loss: 0.823683  [ 1312/ 3500]\n",
      "loss: 0.734035  [ 1632/ 3500]\n",
      "loss: 0.736734  [ 1952/ 3500]\n",
      "loss: 0.764773  [ 2272/ 3500]\n",
      "loss: 0.830472  [ 2592/ 3500]\n",
      "loss: 0.726243  [ 2912/ 3500]\n",
      "loss: 0.975470  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.859289 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.860985  [   32/ 3500]\n",
      "loss: 0.919175  [  352/ 3500]\n",
      "loss: 0.914802  [  672/ 3500]\n",
      "loss: 0.561278  [  992/ 3500]\n",
      "loss: 0.868582  [ 1312/ 3500]\n",
      "loss: 0.972123  [ 1632/ 3500]\n",
      "loss: 1.114378  [ 1952/ 3500]\n",
      "loss: 0.626942  [ 2272/ 3500]\n",
      "loss: 0.782820  [ 2592/ 3500]\n",
      "loss: 0.953278  [ 2912/ 3500]\n",
      "loss: 0.878485  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.863185 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.765409  [   32/ 3500]\n",
      "loss: 0.731061  [  352/ 3500]\n",
      "loss: 0.583946  [  672/ 3500]\n",
      "loss: 0.853926  [  992/ 3500]\n",
      "loss: 0.931575  [ 1312/ 3500]\n",
      "loss: 0.637808  [ 1632/ 3500]\n",
      "loss: 0.865358  [ 1952/ 3500]\n",
      "loss: 1.028111  [ 2272/ 3500]\n",
      "loss: 0.691275  [ 2592/ 3500]\n",
      "loss: 0.852257  [ 2912/ 3500]\n",
      "loss: 0.754461  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.849501 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.953038  [   32/ 3500]\n",
      "loss: 0.711291  [  352/ 3500]\n",
      "loss: 0.775852  [  672/ 3500]\n",
      "loss: 0.833396  [  992/ 3500]\n",
      "loss: 0.775027  [ 1312/ 3500]\n",
      "loss: 1.099582  [ 1632/ 3500]\n",
      "loss: 0.704733  [ 1952/ 3500]\n",
      "loss: 0.757579  [ 2272/ 3500]\n",
      "loss: 0.941620  [ 2592/ 3500]\n",
      "loss: 0.493810  [ 2912/ 3500]\n",
      "loss: 0.640114  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.847234 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.605975  [   32/ 3500]\n",
      "loss: 0.908434  [  352/ 3500]\n",
      "loss: 0.687542  [  672/ 3500]\n",
      "loss: 0.880251  [  992/ 3500]\n",
      "loss: 0.689248  [ 1312/ 3500]\n",
      "loss: 0.851710  [ 1632/ 3500]\n",
      "loss: 0.753496  [ 1952/ 3500]\n",
      "loss: 0.830736  [ 2272/ 3500]\n",
      "loss: 0.783094  [ 2592/ 3500]\n",
      "loss: 0.758429  [ 2912/ 3500]\n",
      "loss: 0.829136  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.834513 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.885431  [   32/ 3500]\n",
      "loss: 0.875057  [  352/ 3500]\n",
      "loss: 0.836749  [  672/ 3500]\n",
      "loss: 0.861248  [  992/ 3500]\n",
      "loss: 0.600708  [ 1312/ 3500]\n",
      "loss: 0.758712  [ 1632/ 3500]\n",
      "loss: 0.922906  [ 1952/ 3500]\n",
      "loss: 0.631979  [ 2272/ 3500]\n",
      "loss: 0.815609  [ 2592/ 3500]\n",
      "loss: 0.593845  [ 2912/ 3500]\n",
      "loss: 0.885180  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.838859 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.711837  [   32/ 3500]\n",
      "loss: 0.783169  [  352/ 3500]\n",
      "loss: 0.696341  [  672/ 3500]\n",
      "loss: 0.625943  [  992/ 3500]\n",
      "loss: 0.655799  [ 1312/ 3500]\n",
      "loss: 0.723931  [ 1632/ 3500]\n",
      "loss: 0.731788  [ 1952/ 3500]\n",
      "loss: 0.610197  [ 2272/ 3500]\n",
      "loss: 0.758723  [ 2592/ 3500]\n",
      "loss: 0.682531  [ 2912/ 3500]\n",
      "loss: 1.071705  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.825609 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.678445  [   32/ 3500]\n",
      "loss: 0.748586  [  352/ 3500]\n",
      "loss: 0.652309  [  672/ 3500]\n",
      "loss: 0.881576  [  992/ 3500]\n",
      "loss: 0.883557  [ 1312/ 3500]\n",
      "loss: 0.609372  [ 1632/ 3500]\n",
      "loss: 0.898963  [ 1952/ 3500]\n",
      "loss: 0.792764  [ 2272/ 3500]\n",
      "loss: 0.507011  [ 2592/ 3500]\n",
      "loss: 0.886802  [ 2912/ 3500]\n",
      "loss: 0.797899  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.827225 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.588409  [   32/ 3500]\n",
      "loss: 0.734077  [  352/ 3500]\n",
      "loss: 0.826975  [  672/ 3500]\n",
      "loss: 0.675546  [  992/ 3500]\n",
      "loss: 0.995245  [ 1312/ 3500]\n",
      "loss: 0.808105  [ 1632/ 3500]\n",
      "loss: 0.704266  [ 1952/ 3500]\n",
      "loss: 0.704310  [ 2272/ 3500]\n",
      "loss: 0.816669  [ 2592/ 3500]\n",
      "loss: 0.947584  [ 2912/ 3500]\n",
      "loss: 0.876601  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.826984 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.691843  [   32/ 3500]\n",
      "loss: 0.886176  [  352/ 3500]\n",
      "loss: 0.748151  [  672/ 3500]\n",
      "loss: 1.015908  [  992/ 3500]\n",
      "loss: 0.759750  [ 1312/ 3500]\n",
      "loss: 0.588378  [ 1632/ 3500]\n",
      "loss: 0.645072  [ 1952/ 3500]\n",
      "loss: 0.507030  [ 2272/ 3500]\n",
      "loss: 0.616935  [ 2592/ 3500]\n",
      "loss: 0.987769  [ 2912/ 3500]\n",
      "loss: 0.554186  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.822719 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.649530  [   32/ 3500]\n",
      "loss: 0.811702  [  352/ 3500]\n",
      "loss: 0.603140  [  672/ 3500]\n",
      "loss: 0.670427  [  992/ 3500]\n",
      "loss: 0.967037  [ 1312/ 3500]\n",
      "loss: 0.870821  [ 1632/ 3500]\n",
      "loss: 0.578968  [ 1952/ 3500]\n",
      "loss: 0.972027  [ 2272/ 3500]\n",
      "loss: 0.817259  [ 2592/ 3500]\n",
      "loss: 0.908118  [ 2912/ 3500]\n",
      "loss: 0.780176  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.830189 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.849828  [   32/ 3500]\n",
      "loss: 0.572997  [  352/ 3500]\n",
      "loss: 0.875521  [  672/ 3500]\n",
      "loss: 0.873460  [  992/ 3500]\n",
      "loss: 0.493529  [ 1312/ 3500]\n",
      "loss: 1.071475  [ 1632/ 3500]\n",
      "loss: 0.724372  [ 1952/ 3500]\n",
      "loss: 0.672175  [ 2272/ 3500]\n",
      "loss: 0.539669  [ 2592/ 3500]\n",
      "loss: 0.761946  [ 2912/ 3500]\n",
      "loss: 0.640733  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.830830 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.574916  [   32/ 3500]\n",
      "loss: 0.837719  [  352/ 3500]\n",
      "loss: 0.560499  [  672/ 3500]\n",
      "loss: 0.693330  [  992/ 3500]\n",
      "loss: 0.620229  [ 1312/ 3500]\n",
      "loss: 0.866005  [ 1632/ 3500]\n",
      "loss: 0.604230  [ 1952/ 3500]\n",
      "loss: 0.663324  [ 2272/ 3500]\n",
      "loss: 0.809038  [ 2592/ 3500]\n",
      "loss: 0.657562  [ 2912/ 3500]\n",
      "loss: 0.804283  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.818307 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Epochs = 50, Learning Rate = 1e-6\n",
    "model_50_1e_6 = NeuralNetwork_Conv().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_50_1e_6.parameters(), lr=1e-6)\n",
    "\n",
    "epochs = 50\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_6, loss_fn, optimizer))\n",
    "    #print(training_losses)\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_6, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.744815  [   32/ 3500]\n",
      "loss: 0.784333  [  352/ 3500]\n",
      "loss: 0.796148  [  672/ 3500]\n",
      "loss: 0.538265  [  992/ 3500]\n",
      "loss: 0.853080  [ 1312/ 3500]\n",
      "loss: 0.546073  [ 1632/ 3500]\n",
      "loss: 0.742188  [ 1952/ 3500]\n",
      "loss: 0.652877  [ 2272/ 3500]\n",
      "loss: 0.773498  [ 2592/ 3500]\n",
      "loss: 0.620184  [ 2912/ 3500]\n",
      "loss: 0.672294  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.813176 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.895268  [   32/ 3500]\n",
      "loss: 0.810387  [  352/ 3500]\n",
      "loss: 0.877722  [  672/ 3500]\n",
      "loss: 0.752938  [  992/ 3500]\n",
      "loss: 0.812312  [ 1312/ 3500]\n",
      "loss: 0.578486  [ 1632/ 3500]\n",
      "loss: 0.528242  [ 1952/ 3500]\n",
      "loss: 0.668373  [ 2272/ 3500]\n",
      "loss: 0.769957  [ 2592/ 3500]\n",
      "loss: 0.880882  [ 2912/ 3500]\n",
      "loss: 0.900884  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.795591 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.531313  [   32/ 3500]\n",
      "loss: 0.574680  [  352/ 3500]\n",
      "loss: 0.740667  [  672/ 3500]\n",
      "loss: 0.671230  [  992/ 3500]\n",
      "loss: 0.921828  [ 1312/ 3500]\n",
      "loss: 0.766827  [ 1632/ 3500]\n",
      "loss: 0.738612  [ 1952/ 3500]\n",
      "loss: 0.976546  [ 2272/ 3500]\n",
      "loss: 0.621996  [ 2592/ 3500]\n",
      "loss: 0.765271  [ 2912/ 3500]\n",
      "loss: 1.073184  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.805749 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.678188  [   32/ 3500]\n",
      "loss: 0.634345  [  352/ 3500]\n",
      "loss: 0.956102  [  672/ 3500]\n",
      "loss: 0.753662  [  992/ 3500]\n",
      "loss: 0.651924  [ 1312/ 3500]\n",
      "loss: 0.946093  [ 1632/ 3500]\n",
      "loss: 0.694123  [ 1952/ 3500]\n",
      "loss: 0.868048  [ 2272/ 3500]\n",
      "loss: 0.801541  [ 2592/ 3500]\n",
      "loss: 0.862217  [ 2912/ 3500]\n",
      "loss: 0.728209  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.812865 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.788933  [   32/ 3500]\n",
      "loss: 0.687932  [  352/ 3500]\n",
      "loss: 0.642146  [  672/ 3500]\n",
      "loss: 0.902466  [  992/ 3500]\n",
      "loss: 0.544262  [ 1312/ 3500]\n",
      "loss: 0.552681  [ 1632/ 3500]\n",
      "loss: 0.813111  [ 1952/ 3500]\n",
      "loss: 0.763586  [ 2272/ 3500]\n",
      "loss: 0.701501  [ 2592/ 3500]\n",
      "loss: 0.777072  [ 2912/ 3500]\n",
      "loss: 1.161522  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.792066 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.648640  [   32/ 3500]\n",
      "loss: 0.642597  [  352/ 3500]\n",
      "loss: 0.702518  [  672/ 3500]\n",
      "loss: 0.750948  [  992/ 3500]\n",
      "loss: 0.694527  [ 1312/ 3500]\n",
      "loss: 0.674948  [ 1632/ 3500]\n",
      "loss: 0.712870  [ 1952/ 3500]\n",
      "loss: 0.740926  [ 2272/ 3500]\n",
      "loss: 1.039982  [ 2592/ 3500]\n",
      "loss: 0.584981  [ 2912/ 3500]\n",
      "loss: 0.755619  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.789899 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.622921  [   32/ 3500]\n",
      "loss: 0.694652  [  352/ 3500]\n",
      "loss: 1.004291  [  672/ 3500]\n",
      "loss: 0.765720  [  992/ 3500]\n",
      "loss: 0.829856  [ 1312/ 3500]\n",
      "loss: 0.557724  [ 1632/ 3500]\n",
      "loss: 0.810336  [ 1952/ 3500]\n",
      "loss: 0.697328  [ 2272/ 3500]\n",
      "loss: 0.698904  [ 2592/ 3500]\n",
      "loss: 1.002668  [ 2912/ 3500]\n",
      "loss: 0.804066  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.780366 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.725157  [   32/ 3500]\n",
      "loss: 0.598875  [  352/ 3500]\n",
      "loss: 0.769630  [  672/ 3500]\n",
      "loss: 0.619181  [  992/ 3500]\n",
      "loss: 0.672283  [ 1312/ 3500]\n",
      "loss: 0.722812  [ 1632/ 3500]\n",
      "loss: 0.755725  [ 1952/ 3500]\n",
      "loss: 0.768382  [ 2272/ 3500]\n",
      "loss: 0.583136  [ 2592/ 3500]\n",
      "loss: 0.763024  [ 2912/ 3500]\n",
      "loss: 0.921204  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.789506 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.675515  [   32/ 3500]\n",
      "loss: 0.463867  [  352/ 3500]\n",
      "loss: 0.581524  [  672/ 3500]\n",
      "loss: 0.561732  [  992/ 3500]\n",
      "loss: 0.907402  [ 1312/ 3500]\n",
      "loss: 0.492829  [ 1632/ 3500]\n",
      "loss: 0.634116  [ 1952/ 3500]\n",
      "loss: 0.885522  [ 2272/ 3500]\n",
      "loss: 0.677484  [ 2592/ 3500]\n",
      "loss: 0.633902  [ 2912/ 3500]\n",
      "loss: 0.795608  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.778694 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.646533  [   32/ 3500]\n",
      "loss: 0.904980  [  352/ 3500]\n",
      "loss: 0.556889  [  672/ 3500]\n",
      "loss: 0.593615  [  992/ 3500]\n",
      "loss: 0.762228  [ 1312/ 3500]\n",
      "loss: 0.715217  [ 1632/ 3500]\n",
      "loss: 0.541529  [ 1952/ 3500]\n",
      "loss: 0.651564  [ 2272/ 3500]\n",
      "loss: 0.660521  [ 2592/ 3500]\n",
      "loss: 0.612710  [ 2912/ 3500]\n",
      "loss: 0.787196  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.776355 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.690006  [   32/ 3500]\n",
      "loss: 0.675371  [  352/ 3500]\n",
      "loss: 0.646111  [  672/ 3500]\n",
      "loss: 0.601759  [  992/ 3500]\n",
      "loss: 0.479030  [ 1312/ 3500]\n",
      "loss: 0.561328  [ 1632/ 3500]\n",
      "loss: 0.686748  [ 1952/ 3500]\n",
      "loss: 0.557825  [ 2272/ 3500]\n",
      "loss: 0.698753  [ 2592/ 3500]\n",
      "loss: 0.639918  [ 2912/ 3500]\n",
      "loss: 0.661501  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.780398 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.612061  [   32/ 3500]\n",
      "loss: 0.699746  [  352/ 3500]\n",
      "loss: 0.654071  [  672/ 3500]\n",
      "loss: 0.474178  [  992/ 3500]\n",
      "loss: 0.430556  [ 1312/ 3500]\n",
      "loss: 0.792728  [ 1632/ 3500]\n",
      "loss: 0.654518  [ 1952/ 3500]\n",
      "loss: 0.576038  [ 2272/ 3500]\n",
      "loss: 0.665118  [ 2592/ 3500]\n",
      "loss: 0.794696  [ 2912/ 3500]\n",
      "loss: 0.771231  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.772586 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.921046  [   32/ 3500]\n",
      "loss: 0.623084  [  352/ 3500]\n",
      "loss: 0.685873  [  672/ 3500]\n",
      "loss: 0.780308  [  992/ 3500]\n",
      "loss: 0.570336  [ 1312/ 3500]\n",
      "loss: 0.565568  [ 1632/ 3500]\n",
      "loss: 0.486828  [ 1952/ 3500]\n",
      "loss: 0.572518  [ 2272/ 3500]\n",
      "loss: 0.474547  [ 2592/ 3500]\n",
      "loss: 0.653399  [ 2912/ 3500]\n",
      "loss: 0.787062  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.765599 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.792837  [   32/ 3500]\n",
      "loss: 0.680081  [  352/ 3500]\n",
      "loss: 0.789114  [  672/ 3500]\n",
      "loss: 0.558585  [  992/ 3500]\n",
      "loss: 0.585263  [ 1312/ 3500]\n",
      "loss: 0.687784  [ 1632/ 3500]\n",
      "loss: 0.525418  [ 1952/ 3500]\n",
      "loss: 0.639986  [ 2272/ 3500]\n",
      "loss: 0.792482  [ 2592/ 3500]\n",
      "loss: 0.666371  [ 2912/ 3500]\n",
      "loss: 0.548549  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.758069 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.888544  [   32/ 3500]\n",
      "loss: 0.730506  [  352/ 3500]\n",
      "loss: 0.576994  [  672/ 3500]\n",
      "loss: 0.629645  [  992/ 3500]\n",
      "loss: 0.722302  [ 1312/ 3500]\n",
      "loss: 0.673699  [ 1632/ 3500]\n",
      "loss: 0.663753  [ 1952/ 3500]\n",
      "loss: 0.791671  [ 2272/ 3500]\n",
      "loss: 0.658113  [ 2592/ 3500]\n",
      "loss: 0.785242  [ 2912/ 3500]\n",
      "loss: 0.929215  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.761172 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.746160  [   32/ 3500]\n",
      "loss: 0.532904  [  352/ 3500]\n",
      "loss: 0.497204  [  672/ 3500]\n",
      "loss: 0.708404  [  992/ 3500]\n",
      "loss: 0.521717  [ 1312/ 3500]\n",
      "loss: 0.738484  [ 1632/ 3500]\n",
      "loss: 0.597859  [ 1952/ 3500]\n",
      "loss: 0.464350  [ 2272/ 3500]\n",
      "loss: 0.701461  [ 2592/ 3500]\n",
      "loss: 0.771098  [ 2912/ 3500]\n",
      "loss: 0.801696  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.758090 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.917730  [   32/ 3500]\n",
      "loss: 0.736381  [  352/ 3500]\n",
      "loss: 0.668721  [  672/ 3500]\n",
      "loss: 0.522220  [  992/ 3500]\n",
      "loss: 0.515015  [ 1312/ 3500]\n",
      "loss: 0.616911  [ 1632/ 3500]\n",
      "loss: 0.716380  [ 1952/ 3500]\n",
      "loss: 0.640927  [ 2272/ 3500]\n",
      "loss: 0.641264  [ 2592/ 3500]\n",
      "loss: 0.778482  [ 2912/ 3500]\n",
      "loss: 0.812388  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.749534 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.723153  [   32/ 3500]\n",
      "loss: 0.789102  [  352/ 3500]\n",
      "loss: 0.866790  [  672/ 3500]\n",
      "loss: 0.591733  [  992/ 3500]\n",
      "loss: 0.729944  [ 1312/ 3500]\n",
      "loss: 0.880841  [ 1632/ 3500]\n",
      "loss: 0.479649  [ 1952/ 3500]\n",
      "loss: 0.541164  [ 2272/ 3500]\n",
      "loss: 0.788554  [ 2592/ 3500]\n",
      "loss: 0.857058  [ 2912/ 3500]\n",
      "loss: 0.482683  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.758750 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.564647  [   32/ 3500]\n",
      "loss: 0.763274  [  352/ 3500]\n",
      "loss: 0.701881  [  672/ 3500]\n",
      "loss: 0.442901  [  992/ 3500]\n",
      "loss: 0.488292  [ 1312/ 3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.770647  [ 1632/ 3500]\n",
      "loss: 0.675134  [ 1952/ 3500]\n",
      "loss: 0.749802  [ 2272/ 3500]\n",
      "loss: 0.683143  [ 2592/ 3500]\n",
      "loss: 0.757799  [ 2912/ 3500]\n",
      "loss: 0.585979  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.747357 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.750783  [   32/ 3500]\n",
      "loss: 0.550370  [  352/ 3500]\n",
      "loss: 0.575149  [  672/ 3500]\n",
      "loss: 0.652915  [  992/ 3500]\n",
      "loss: 0.603174  [ 1312/ 3500]\n",
      "loss: 0.827622  [ 1632/ 3500]\n",
      "loss: 0.645607  [ 1952/ 3500]\n",
      "loss: 0.507632  [ 2272/ 3500]\n",
      "loss: 0.602803  [ 2592/ 3500]\n",
      "loss: 0.745216  [ 2912/ 3500]\n",
      "loss: 0.508864  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.750417 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.706477  [   32/ 3500]\n",
      "loss: 0.627913  [  352/ 3500]\n",
      "loss: 0.635617  [  672/ 3500]\n",
      "loss: 0.749221  [  992/ 3500]\n",
      "loss: 0.512640  [ 1312/ 3500]\n",
      "loss: 0.635308  [ 1632/ 3500]\n",
      "loss: 0.507131  [ 1952/ 3500]\n",
      "loss: 0.697194  [ 2272/ 3500]\n",
      "loss: 0.823831  [ 2592/ 3500]\n",
      "loss: 0.783348  [ 2912/ 3500]\n",
      "loss: 0.581478  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.754331 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.753738  [   32/ 3500]\n",
      "loss: 0.732904  [  352/ 3500]\n",
      "loss: 0.549724  [  672/ 3500]\n",
      "loss: 0.795251  [  992/ 3500]\n",
      "loss: 0.649381  [ 1312/ 3500]\n",
      "loss: 0.760930  [ 1632/ 3500]\n",
      "loss: 0.595770  [ 1952/ 3500]\n",
      "loss: 0.488653  [ 2272/ 3500]\n",
      "loss: 0.743715  [ 2592/ 3500]\n",
      "loss: 0.539566  [ 2912/ 3500]\n",
      "loss: 0.463586  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.760927 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.574236  [   32/ 3500]\n",
      "loss: 0.748570  [  352/ 3500]\n",
      "loss: 0.890428  [  672/ 3500]\n",
      "loss: 0.881421  [  992/ 3500]\n",
      "loss: 0.547414  [ 1312/ 3500]\n",
      "loss: 0.672178  [ 1632/ 3500]\n",
      "loss: 0.549165  [ 1952/ 3500]\n",
      "loss: 0.813588  [ 2272/ 3500]\n",
      "loss: 0.453203  [ 2592/ 3500]\n",
      "loss: 0.677855  [ 2912/ 3500]\n",
      "loss: 0.702089  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.741385 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.552829  [   32/ 3500]\n",
      "loss: 0.837868  [  352/ 3500]\n",
      "loss: 0.608404  [  672/ 3500]\n",
      "loss: 0.584414  [  992/ 3500]\n",
      "loss: 0.708102  [ 1312/ 3500]\n",
      "loss: 0.886572  [ 1632/ 3500]\n",
      "loss: 0.784794  [ 1952/ 3500]\n",
      "loss: 0.687417  [ 2272/ 3500]\n",
      "loss: 0.622442  [ 2592/ 3500]\n",
      "loss: 0.671653  [ 2912/ 3500]\n",
      "loss: 0.566513  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.748287 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.915847  [   32/ 3500]\n",
      "loss: 0.515193  [  352/ 3500]\n",
      "loss: 0.863195  [  672/ 3500]\n",
      "loss: 0.727910  [  992/ 3500]\n",
      "loss: 0.515570  [ 1312/ 3500]\n",
      "loss: 0.551471  [ 1632/ 3500]\n",
      "loss: 0.704962  [ 1952/ 3500]\n",
      "loss: 0.614270  [ 2272/ 3500]\n",
      "loss: 0.457614  [ 2592/ 3500]\n",
      "loss: 0.584285  [ 2912/ 3500]\n",
      "loss: 0.774825  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.731273 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.669750  [   32/ 3500]\n",
      "loss: 0.493512  [  352/ 3500]\n",
      "loss: 0.463502  [  672/ 3500]\n",
      "loss: 0.607652  [  992/ 3500]\n",
      "loss: 0.566349  [ 1312/ 3500]\n",
      "loss: 0.663074  [ 1632/ 3500]\n",
      "loss: 0.853041  [ 1952/ 3500]\n",
      "loss: 0.468521  [ 2272/ 3500]\n",
      "loss: 0.792304  [ 2592/ 3500]\n",
      "loss: 0.699423  [ 2912/ 3500]\n",
      "loss: 0.554303  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.726591 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.471934  [   32/ 3500]\n",
      "loss: 0.390138  [  352/ 3500]\n",
      "loss: 0.533430  [  672/ 3500]\n",
      "loss: 0.608353  [  992/ 3500]\n",
      "loss: 0.494641  [ 1312/ 3500]\n",
      "loss: 0.567844  [ 1632/ 3500]\n",
      "loss: 0.481655  [ 1952/ 3500]\n",
      "loss: 0.814559  [ 2272/ 3500]\n",
      "loss: 0.594509  [ 2592/ 3500]\n",
      "loss: 0.600923  [ 2912/ 3500]\n",
      "loss: 0.593687  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.732949 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.693624  [   32/ 3500]\n",
      "loss: 0.678972  [  352/ 3500]\n",
      "loss: 0.521255  [  672/ 3500]\n",
      "loss: 0.559649  [  992/ 3500]\n",
      "loss: 0.598153  [ 1312/ 3500]\n",
      "loss: 0.511649  [ 1632/ 3500]\n",
      "loss: 0.375259  [ 1952/ 3500]\n",
      "loss: 0.626432  [ 2272/ 3500]\n",
      "loss: 0.648332  [ 2592/ 3500]\n",
      "loss: 0.601010  [ 2912/ 3500]\n",
      "loss: 0.587236  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.728637 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.715101  [   32/ 3500]\n",
      "loss: 0.837861  [  352/ 3500]\n",
      "loss: 0.585192  [  672/ 3500]\n",
      "loss: 0.644900  [  992/ 3500]\n",
      "loss: 0.488105  [ 1312/ 3500]\n",
      "loss: 0.578229  [ 1632/ 3500]\n",
      "loss: 1.029692  [ 1952/ 3500]\n",
      "loss: 0.587670  [ 2272/ 3500]\n",
      "loss: 0.796511  [ 2592/ 3500]\n",
      "loss: 0.707763  [ 2912/ 3500]\n",
      "loss: 0.734609  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.721232 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.767346  [   32/ 3500]\n",
      "loss: 0.564471  [  352/ 3500]\n",
      "loss: 0.749200  [  672/ 3500]\n",
      "loss: 0.636716  [  992/ 3500]\n",
      "loss: 0.389552  [ 1312/ 3500]\n",
      "loss: 0.536178  [ 1632/ 3500]\n",
      "loss: 0.752948  [ 1952/ 3500]\n",
      "loss: 0.533191  [ 2272/ 3500]\n",
      "loss: 0.738920  [ 2592/ 3500]\n",
      "loss: 0.637416  [ 2912/ 3500]\n",
      "loss: 0.649225  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.720774 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.752424  [   32/ 3500]\n",
      "loss: 0.919649  [  352/ 3500]\n",
      "loss: 0.484479  [  672/ 3500]\n",
      "loss: 0.546191  [  992/ 3500]\n",
      "loss: 0.605896  [ 1312/ 3500]\n",
      "loss: 0.492984  [ 1632/ 3500]\n",
      "loss: 0.368377  [ 1952/ 3500]\n",
      "loss: 0.539389  [ 2272/ 3500]\n",
      "loss: 0.823944  [ 2592/ 3500]\n",
      "loss: 0.782200  [ 2912/ 3500]\n",
      "loss: 0.511178  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.720531 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.425332  [   32/ 3500]\n",
      "loss: 0.693442  [  352/ 3500]\n",
      "loss: 0.426899  [  672/ 3500]\n",
      "loss: 0.805817  [  992/ 3500]\n",
      "loss: 0.693640  [ 1312/ 3500]\n",
      "loss: 0.803649  [ 1632/ 3500]\n",
      "loss: 0.553057  [ 1952/ 3500]\n",
      "loss: 0.522187  [ 2272/ 3500]\n",
      "loss: 0.818632  [ 2592/ 3500]\n",
      "loss: 0.642806  [ 2912/ 3500]\n",
      "loss: 0.639199  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.728454 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.639151  [   32/ 3500]\n",
      "loss: 0.554329  [  352/ 3500]\n",
      "loss: 0.572106  [  672/ 3500]\n",
      "loss: 0.636760  [  992/ 3500]\n",
      "loss: 0.725895  [ 1312/ 3500]\n",
      "loss: 0.640744  [ 1632/ 3500]\n",
      "loss: 0.827022  [ 1952/ 3500]\n",
      "loss: 0.376671  [ 2272/ 3500]\n",
      "loss: 0.683047  [ 2592/ 3500]\n",
      "loss: 0.807322  [ 2912/ 3500]\n",
      "loss: 0.596305  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.722552 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.579010  [   32/ 3500]\n",
      "loss: 0.659854  [  352/ 3500]\n",
      "loss: 0.655040  [  672/ 3500]\n",
      "loss: 0.851351  [  992/ 3500]\n",
      "loss: 0.710811  [ 1312/ 3500]\n",
      "loss: 0.508377  [ 1632/ 3500]\n",
      "loss: 0.686074  [ 1952/ 3500]\n",
      "loss: 0.644772  [ 2272/ 3500]\n",
      "loss: 0.828945  [ 2592/ 3500]\n",
      "loss: 0.678840  [ 2912/ 3500]\n",
      "loss: 0.585266  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.718348 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.720862  [   32/ 3500]\n",
      "loss: 0.503896  [  352/ 3500]\n",
      "loss: 0.593925  [  672/ 3500]\n",
      "loss: 0.545930  [  992/ 3500]\n",
      "loss: 0.531338  [ 1312/ 3500]\n",
      "loss: 0.677455  [ 1632/ 3500]\n",
      "loss: 0.808345  [ 1952/ 3500]\n",
      "loss: 0.527837  [ 2272/ 3500]\n",
      "loss: 0.517015  [ 2592/ 3500]\n",
      "loss: 0.521576  [ 2912/ 3500]\n",
      "loss: 0.414189  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.723655 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.609396  [   32/ 3500]\n",
      "loss: 0.609106  [  352/ 3500]\n",
      "loss: 0.798854  [  672/ 3500]\n",
      "loss: 0.627593  [  992/ 3500]\n",
      "loss: 0.333324  [ 1312/ 3500]\n",
      "loss: 0.714514  [ 1632/ 3500]\n",
      "loss: 0.649256  [ 1952/ 3500]\n",
      "loss: 0.667905  [ 2272/ 3500]\n",
      "loss: 0.624440  [ 2592/ 3500]\n",
      "loss: 0.568546  [ 2912/ 3500]\n",
      "loss: 0.602180  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.720159 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.481989  [   32/ 3500]\n",
      "loss: 0.531254  [  352/ 3500]\n",
      "loss: 0.833304  [  672/ 3500]\n",
      "loss: 0.731125  [  992/ 3500]\n",
      "loss: 0.604144  [ 1312/ 3500]\n",
      "loss: 0.541820  [ 1632/ 3500]\n",
      "loss: 0.547853  [ 1952/ 3500]\n",
      "loss: 0.608935  [ 2272/ 3500]\n",
      "loss: 0.618636  [ 2592/ 3500]\n",
      "loss: 0.624859  [ 2912/ 3500]\n",
      "loss: 0.989917  [ 3232/ 3500]\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.718621 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.634694  [   32/ 3500]\n",
      "loss: 0.640651  [  352/ 3500]\n",
      "loss: 0.576558  [  672/ 3500]\n",
      "loss: 0.522746  [  992/ 3500]\n",
      "loss: 0.780063  [ 1312/ 3500]\n",
      "loss: 0.480247  [ 1632/ 3500]\n",
      "loss: 0.976783  [ 1952/ 3500]\n",
      "loss: 0.500753  [ 2272/ 3500]\n",
      "loss: 0.502730  [ 2592/ 3500]\n",
      "loss: 0.641646  [ 2912/ 3500]\n",
      "loss: 0.552788  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.714139 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.608141  [   32/ 3500]\n",
      "loss: 0.640188  [  352/ 3500]\n",
      "loss: 0.452838  [  672/ 3500]\n",
      "loss: 0.527109  [  992/ 3500]\n",
      "loss: 0.564656  [ 1312/ 3500]\n",
      "loss: 0.768300  [ 1632/ 3500]\n",
      "loss: 0.650786  [ 1952/ 3500]\n",
      "loss: 0.730322  [ 2272/ 3500]\n",
      "loss: 0.665427  [ 2592/ 3500]\n",
      "loss: 0.499889  [ 2912/ 3500]\n",
      "loss: 0.873287  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.731773 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.651397  [   32/ 3500]\n",
      "loss: 0.659471  [  352/ 3500]\n",
      "loss: 0.944826  [  672/ 3500]\n",
      "loss: 0.511436  [  992/ 3500]\n",
      "loss: 0.946678  [ 1312/ 3500]\n",
      "loss: 0.502344  [ 1632/ 3500]\n",
      "loss: 0.635348  [ 1952/ 3500]\n",
      "loss: 0.483278  [ 2272/ 3500]\n",
      "loss: 0.800838  [ 2592/ 3500]\n",
      "loss: 0.393704  [ 2912/ 3500]\n",
      "loss: 0.624929  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.721782 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.552397  [   32/ 3500]\n",
      "loss: 0.643034  [  352/ 3500]\n",
      "loss: 1.028692  [  672/ 3500]\n",
      "loss: 0.483209  [  992/ 3500]\n",
      "loss: 0.653873  [ 1312/ 3500]\n",
      "loss: 0.762898  [ 1632/ 3500]\n",
      "loss: 0.619306  [ 1952/ 3500]\n",
      "loss: 0.549152  [ 2272/ 3500]\n",
      "loss: 0.747847  [ 2592/ 3500]\n",
      "loss: 0.693063  [ 2912/ 3500]\n",
      "loss: 0.523133  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.710338 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.754280  [   32/ 3500]\n",
      "loss: 0.680947  [  352/ 3500]\n",
      "loss: 0.595512  [  672/ 3500]\n",
      "loss: 0.576384  [  992/ 3500]\n",
      "loss: 0.471728  [ 1312/ 3500]\n",
      "loss: 0.622334  [ 1632/ 3500]\n",
      "loss: 0.595670  [ 1952/ 3500]\n",
      "loss: 0.769650  [ 2272/ 3500]\n",
      "loss: 0.392101  [ 2592/ 3500]\n",
      "loss: 0.442381  [ 2912/ 3500]\n",
      "loss: 0.438274  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.703989 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.601948  [   32/ 3500]\n",
      "loss: 0.498955  [  352/ 3500]\n",
      "loss: 0.760408  [  672/ 3500]\n",
      "loss: 0.615924  [  992/ 3500]\n",
      "loss: 0.919173  [ 1312/ 3500]\n",
      "loss: 0.606167  [ 1632/ 3500]\n",
      "loss: 0.799700  [ 1952/ 3500]\n",
      "loss: 0.534788  [ 2272/ 3500]\n",
      "loss: 0.572814  [ 2592/ 3500]\n",
      "loss: 0.700572  [ 2912/ 3500]\n",
      "loss: 0.470826  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.715401 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.594812  [   32/ 3500]\n",
      "loss: 0.769441  [  352/ 3500]\n",
      "loss: 0.772961  [  672/ 3500]\n",
      "loss: 0.625305  [  992/ 3500]\n",
      "loss: 0.498454  [ 1312/ 3500]\n",
      "loss: 0.479778  [ 1632/ 3500]\n",
      "loss: 0.470915  [ 1952/ 3500]\n",
      "loss: 0.738626  [ 2272/ 3500]\n",
      "loss: 0.649808  [ 2592/ 3500]\n",
      "loss: 0.595894  [ 2912/ 3500]\n",
      "loss: 0.613331  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.714125 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.541843  [   32/ 3500]\n",
      "loss: 0.658413  [  352/ 3500]\n",
      "loss: 0.627854  [  672/ 3500]\n",
      "loss: 0.732208  [  992/ 3500]\n",
      "loss: 0.496493  [ 1312/ 3500]\n",
      "loss: 0.471703  [ 1632/ 3500]\n",
      "loss: 0.619434  [ 1952/ 3500]\n",
      "loss: 0.421506  [ 2272/ 3500]\n",
      "loss: 0.531273  [ 2592/ 3500]\n",
      "loss: 0.486440  [ 2912/ 3500]\n",
      "loss: 0.683679  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.701326 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.684759  [   32/ 3500]\n",
      "loss: 0.715190  [  352/ 3500]\n",
      "loss: 0.752659  [  672/ 3500]\n",
      "loss: 0.470720  [  992/ 3500]\n",
      "loss: 0.611586  [ 1312/ 3500]\n",
      "loss: 0.642101  [ 1632/ 3500]\n",
      "loss: 0.720368  [ 1952/ 3500]\n",
      "loss: 0.525742  [ 2272/ 3500]\n",
      "loss: 0.532006  [ 2592/ 3500]\n",
      "loss: 0.600652  [ 2912/ 3500]\n",
      "loss: 0.515433  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.697631 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.935874  [   32/ 3500]\n",
      "loss: 0.747684  [  352/ 3500]\n",
      "loss: 0.744990  [  672/ 3500]\n",
      "loss: 0.545007  [  992/ 3500]\n",
      "loss: 0.868061  [ 1312/ 3500]\n",
      "loss: 0.546906  [ 1632/ 3500]\n",
      "loss: 0.667546  [ 1952/ 3500]\n",
      "loss: 0.537308  [ 2272/ 3500]\n",
      "loss: 0.447221  [ 2592/ 3500]\n",
      "loss: 0.609951  [ 2912/ 3500]\n",
      "loss: 0.561356  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.700577 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.486504  [   32/ 3500]\n",
      "loss: 0.704277  [  352/ 3500]\n",
      "loss: 0.758055  [  672/ 3500]\n",
      "loss: 0.409778  [  992/ 3500]\n",
      "loss: 0.889898  [ 1312/ 3500]\n",
      "loss: 0.795703  [ 1632/ 3500]\n",
      "loss: 0.728908  [ 1952/ 3500]\n",
      "loss: 0.597205  [ 2272/ 3500]\n",
      "loss: 0.590998  [ 2592/ 3500]\n",
      "loss: 0.653733  [ 2912/ 3500]\n",
      "loss: 0.497842  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.701811 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.628721  [   32/ 3500]\n",
      "loss: 0.521452  [  352/ 3500]\n",
      "loss: 0.623300  [  672/ 3500]\n",
      "loss: 0.517128  [  992/ 3500]\n",
      "loss: 0.818056  [ 1312/ 3500]\n",
      "loss: 0.728721  [ 1632/ 3500]\n",
      "loss: 0.525151  [ 1952/ 3500]\n",
      "loss: 0.569964  [ 2272/ 3500]\n",
      "loss: 0.767752  [ 2592/ 3500]\n",
      "loss: 0.717692  [ 2912/ 3500]\n",
      "loss: 0.835771  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.696564 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (49,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_51284/2635469260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_50_1e_6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2757\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \"\"\"\n\u001b[0;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (49,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Continued\n",
    "#Epochs = 50, Learning Rate = 1e-6\n",
    "#model_50_1e_6 = NeuralNetwork_Conv().to(device)\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model_50_1e_6.parameters(), lr=1e-6)\n",
    "\n",
    "epochs = 99\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(50, epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_6, loss_fn, optimizer))\n",
    "    #print(training_losses)\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_6, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(len(validation_losses), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(len(training_losses), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1648864865414,
     "user": {
      "displayName": "Vedant Gannu",
      "userId": "02485315243456304443"
     },
     "user_tz": 240
    },
    "id": "WfDMcp6ElFzR"
   },
   "outputs": [],
   "source": [
    "#Save this model to CNN_50_1e_6.pth\n",
    "torch.save(model_50_1e_6.state_dict(), \"./CNN_50_1e_6.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rPZze8StADJQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_Conv, self).__init__()\n",
    "        #Images are R=240 by C=360\n",
    "        self.conv_stack = nn.Sequential(\n",
    "           nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        #  Resulting image should be 60*90\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #Padding preserves shape, but 2 max pools divides dims by 4\n",
    "            nn.Linear(60*90*32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc_stack(self.conv_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 2.228713  [   32/ 3500]\n",
      "loss: 1.925373  [  352/ 3500]\n",
      "loss: 1.753791  [  672/ 3500]\n",
      "loss: 1.656554  [  992/ 3500]\n",
      "loss: 1.456468  [ 1312/ 3500]\n",
      "loss: 1.360551  [ 1632/ 3500]\n",
      "loss: 1.447381  [ 1952/ 3500]\n",
      "loss: 1.369355  [ 2272/ 3500]\n",
      "loss: 1.325989  [ 2592/ 3500]\n",
      "loss: 1.386948  [ 2912/ 3500]\n",
      "loss: 1.215557  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.252166 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.214299  [   32/ 3500]\n",
      "loss: 1.308741  [  352/ 3500]\n",
      "loss: 1.491348  [  672/ 3500]\n",
      "loss: 1.362225  [  992/ 3500]\n",
      "loss: 0.983700  [ 1312/ 3500]\n",
      "loss: 1.281181  [ 1632/ 3500]\n",
      "loss: 0.985739  [ 1952/ 3500]\n",
      "loss: 0.905824  [ 2272/ 3500]\n",
      "loss: 1.020942  [ 2592/ 3500]\n",
      "loss: 0.710615  [ 2912/ 3500]\n",
      "loss: 1.553693  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 1.062211 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.712148  [   32/ 3500]\n",
      "loss: 1.114568  [  352/ 3500]\n",
      "loss: 0.968296  [  672/ 3500]\n",
      "loss: 1.206389  [  992/ 3500]\n",
      "loss: 0.893196  [ 1312/ 3500]\n",
      "loss: 1.037779  [ 1632/ 3500]\n",
      "loss: 0.941446  [ 1952/ 3500]\n",
      "loss: 1.111016  [ 2272/ 3500]\n",
      "loss: 0.813101  [ 2592/ 3500]\n",
      "loss: 0.832388  [ 2912/ 3500]\n",
      "loss: 0.796313  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.110832 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.273037  [   32/ 3500]\n",
      "loss: 0.794027  [  352/ 3500]\n",
      "loss: 0.981382  [  672/ 3500]\n",
      "loss: 0.919354  [  992/ 3500]\n",
      "loss: 0.558507  [ 1312/ 3500]\n",
      "loss: 0.836982  [ 1632/ 3500]\n",
      "loss: 0.719254  [ 1952/ 3500]\n",
      "loss: 0.797166  [ 2272/ 3500]\n",
      "loss: 0.815982  [ 2592/ 3500]\n",
      "loss: 0.850494  [ 2912/ 3500]\n",
      "loss: 0.834967  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.939965 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.837255  [   32/ 3500]\n",
      "loss: 0.848056  [  352/ 3500]\n",
      "loss: 0.806666  [  672/ 3500]\n",
      "loss: 0.598175  [  992/ 3500]\n",
      "loss: 0.935133  [ 1312/ 3500]\n",
      "loss: 0.902862  [ 1632/ 3500]\n",
      "loss: 0.823727  [ 1952/ 3500]\n",
      "loss: 0.773760  [ 2272/ 3500]\n",
      "loss: 0.798944  [ 2592/ 3500]\n",
      "loss: 1.073751  [ 2912/ 3500]\n",
      "loss: 0.795247  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.834356 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.746474  [   32/ 3500]\n",
      "loss: 0.514085  [  352/ 3500]\n",
      "loss: 0.603978  [  672/ 3500]\n",
      "loss: 0.750244  [  992/ 3500]\n",
      "loss: 0.805240  [ 1312/ 3500]\n",
      "loss: 0.624275  [ 1632/ 3500]\n",
      "loss: 1.017252  [ 1952/ 3500]\n",
      "loss: 0.782429  [ 2272/ 3500]\n",
      "loss: 0.782534  [ 2592/ 3500]\n",
      "loss: 0.664335  [ 2912/ 3500]\n",
      "loss: 0.741204  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 48.3%, Avg loss: 1.209142 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.755066  [   32/ 3500]\n",
      "loss: 1.042844  [  352/ 3500]\n",
      "loss: 0.753111  [  672/ 3500]\n",
      "loss: 0.521982  [  992/ 3500]\n",
      "loss: 0.846716  [ 1312/ 3500]\n",
      "loss: 0.727449  [ 1632/ 3500]\n",
      "loss: 0.854635  [ 1952/ 3500]\n",
      "loss: 0.717746  [ 2272/ 3500]\n",
      "loss: 0.592520  [ 2592/ 3500]\n",
      "loss: 0.530845  [ 2912/ 3500]\n",
      "loss: 0.661234  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.808612 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.631651  [   32/ 3500]\n",
      "loss: 0.571260  [  352/ 3500]\n",
      "loss: 0.587119  [  672/ 3500]\n",
      "loss: 0.529355  [  992/ 3500]\n",
      "loss: 0.640734  [ 1312/ 3500]\n",
      "loss: 0.660743  [ 1632/ 3500]\n",
      "loss: 1.011900  [ 1952/ 3500]\n",
      "loss: 0.540105  [ 2272/ 3500]\n",
      "loss: 0.835143  [ 2592/ 3500]\n",
      "loss: 0.803572  [ 2912/ 3500]\n",
      "loss: 0.447317  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.176797 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.005647  [   32/ 3500]\n",
      "loss: 0.585650  [  352/ 3500]\n",
      "loss: 0.471107  [  672/ 3500]\n",
      "loss: 0.754398  [  992/ 3500]\n",
      "loss: 0.691923  [ 1312/ 3500]\n",
      "loss: 0.562774  [ 1632/ 3500]\n",
      "loss: 0.843001  [ 1952/ 3500]\n",
      "loss: 0.699497  [ 2272/ 3500]\n",
      "loss: 0.419792  [ 2592/ 3500]\n",
      "loss: 0.474902  [ 2912/ 3500]\n",
      "loss: 0.898727  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.817339 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.528601  [   32/ 3500]\n",
      "loss: 0.466840  [  352/ 3500]\n",
      "loss: 0.527018  [  672/ 3500]\n",
      "loss: 0.635429  [  992/ 3500]\n",
      "loss: 0.691921  [ 1312/ 3500]\n",
      "loss: 0.428577  [ 1632/ 3500]\n",
      "loss: 0.577474  [ 1952/ 3500]\n",
      "loss: 1.023116  [ 2272/ 3500]\n",
      "loss: 0.322139  [ 2592/ 3500]\n",
      "loss: 0.779210  [ 2912/ 3500]\n",
      "loss: 1.238608  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.692807 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.582737  [   32/ 3500]\n",
      "loss: 0.394842  [  352/ 3500]\n",
      "loss: 0.602471  [  672/ 3500]\n",
      "loss: 0.475747  [  992/ 3500]\n",
      "loss: 0.558801  [ 1312/ 3500]\n",
      "loss: 0.620283  [ 1632/ 3500]\n",
      "loss: 0.615597  [ 1952/ 3500]\n",
      "loss: 0.553080  [ 2272/ 3500]\n",
      "loss: 0.533175  [ 2592/ 3500]\n",
      "loss: 0.498412  [ 2912/ 3500]\n",
      "loss: 0.747057  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.731802 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.851340  [   32/ 3500]\n",
      "loss: 0.545407  [  352/ 3500]\n",
      "loss: 0.635849  [  672/ 3500]\n",
      "loss: 0.407154  [  992/ 3500]\n",
      "loss: 0.420443  [ 1312/ 3500]\n",
      "loss: 0.981671  [ 1632/ 3500]\n",
      "loss: 0.512356  [ 1952/ 3500]\n",
      "loss: 0.682819  [ 2272/ 3500]\n",
      "loss: 0.551284  [ 2592/ 3500]\n",
      "loss: 0.377853  [ 2912/ 3500]\n",
      "loss: 0.484153  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.753560 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.638381  [   32/ 3500]\n",
      "loss: 0.510122  [  352/ 3500]\n",
      "loss: 0.469829  [  672/ 3500]\n",
      "loss: 0.640122  [  992/ 3500]\n",
      "loss: 0.678895  [ 1312/ 3500]\n",
      "loss: 0.646022  [ 1632/ 3500]\n",
      "loss: 0.405502  [ 1952/ 3500]\n",
      "loss: 0.418504  [ 2272/ 3500]\n",
      "loss: 0.552497  [ 2592/ 3500]\n",
      "loss: 0.334020  [ 2912/ 3500]\n",
      "loss: 0.402847  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.754175 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.490474  [   32/ 3500]\n",
      "loss: 0.503615  [  352/ 3500]\n",
      "loss: 0.869987  [  672/ 3500]\n",
      "loss: 0.614617  [  992/ 3500]\n",
      "loss: 0.638249  [ 1312/ 3500]\n",
      "loss: 0.465168  [ 1632/ 3500]\n",
      "loss: 0.332320  [ 1952/ 3500]\n",
      "loss: 0.414028  [ 2272/ 3500]\n",
      "loss: 0.401490  [ 2592/ 3500]\n",
      "loss: 0.554997  [ 2912/ 3500]\n",
      "loss: 0.622796  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.725618 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.576833  [   32/ 3500]\n",
      "loss: 0.528049  [  352/ 3500]\n",
      "loss: 0.524922  [  672/ 3500]\n",
      "loss: 0.541353  [  992/ 3500]\n",
      "loss: 0.503167  [ 1312/ 3500]\n",
      "loss: 0.549645  [ 1632/ 3500]\n",
      "loss: 0.449845  [ 1952/ 3500]\n",
      "loss: 0.450786  [ 2272/ 3500]\n",
      "loss: 0.594777  [ 2592/ 3500]\n",
      "loss: 0.616823  [ 2912/ 3500]\n",
      "loss: 0.575266  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.633099 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.439837  [   32/ 3500]\n",
      "loss: 0.507029  [  352/ 3500]\n",
      "loss: 0.410815  [  672/ 3500]\n",
      "loss: 0.295603  [  992/ 3500]\n",
      "loss: 0.571634  [ 1312/ 3500]\n",
      "loss: 0.384240  [ 1632/ 3500]\n",
      "loss: 0.709309  [ 1952/ 3500]\n",
      "loss: 0.407004  [ 2272/ 3500]\n",
      "loss: 0.521938  [ 2592/ 3500]\n",
      "loss: 0.424203  [ 2912/ 3500]\n",
      "loss: 0.406334  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.706984 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.456994  [   32/ 3500]\n",
      "loss: 0.376637  [  352/ 3500]\n",
      "loss: 0.319076  [  672/ 3500]\n",
      "loss: 0.398102  [  992/ 3500]\n",
      "loss: 0.530921  [ 1312/ 3500]\n",
      "loss: 0.469363  [ 1632/ 3500]\n",
      "loss: 0.581968  [ 1952/ 3500]\n",
      "loss: 0.621885  [ 2272/ 3500]\n",
      "loss: 0.391609  [ 2592/ 3500]\n",
      "loss: 0.301238  [ 2912/ 3500]\n",
      "loss: 0.417012  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.793431 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.642849  [   32/ 3500]\n",
      "loss: 0.475876  [  352/ 3500]\n",
      "loss: 0.569850  [  672/ 3500]\n",
      "loss: 0.352916  [  992/ 3500]\n",
      "loss: 0.391864  [ 1312/ 3500]\n",
      "loss: 0.462957  [ 1632/ 3500]\n",
      "loss: 0.518617  [ 1952/ 3500]\n",
      "loss: 0.829849  [ 2272/ 3500]\n",
      "loss: 0.361296  [ 2592/ 3500]\n",
      "loss: 0.700704  [ 2912/ 3500]\n",
      "loss: 0.373945  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.706226 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.432228  [   32/ 3500]\n",
      "loss: 0.576433  [  352/ 3500]\n",
      "loss: 0.488045  [  672/ 3500]\n",
      "loss: 0.573654  [  992/ 3500]\n",
      "loss: 0.608585  [ 1312/ 3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.261918  [ 1632/ 3500]\n",
      "loss: 0.388307  [ 1952/ 3500]\n",
      "loss: 0.559541  [ 2272/ 3500]\n",
      "loss: 0.440468  [ 2592/ 3500]\n",
      "loss: 0.612608  [ 2912/ 3500]\n",
      "loss: 0.412810  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.812628 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.452174  [   32/ 3500]\n",
      "loss: 0.347986  [  352/ 3500]\n",
      "loss: 0.520616  [  672/ 3500]\n",
      "loss: 0.407887  [  992/ 3500]\n",
      "loss: 0.515862  [ 1312/ 3500]\n",
      "loss: 0.531510  [ 1632/ 3500]\n",
      "loss: 0.413064  [ 1952/ 3500]\n",
      "loss: 0.149304  [ 2272/ 3500]\n",
      "loss: 0.503431  [ 2592/ 3500]\n",
      "loss: 0.410302  [ 2912/ 3500]\n",
      "loss: 0.336308  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.608463 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.318490  [   32/ 3500]\n",
      "loss: 0.340070  [  352/ 3500]\n",
      "loss: 0.809732  [  672/ 3500]\n",
      "loss: 0.563313  [  992/ 3500]\n",
      "loss: 0.314242  [ 1312/ 3500]\n",
      "loss: 0.457157  [ 1632/ 3500]\n",
      "loss: 0.405936  [ 1952/ 3500]\n",
      "loss: 0.559810  [ 2272/ 3500]\n",
      "loss: 0.360579  [ 2592/ 3500]\n",
      "loss: 0.326528  [ 2912/ 3500]\n",
      "loss: 0.394559  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.587412 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.489993  [   32/ 3500]\n",
      "loss: 0.377632  [  352/ 3500]\n",
      "loss: 0.321104  [  672/ 3500]\n",
      "loss: 0.600418  [  992/ 3500]\n",
      "loss: 0.426930  [ 1312/ 3500]\n",
      "loss: 0.354117  [ 1632/ 3500]\n",
      "loss: 0.386190  [ 1952/ 3500]\n",
      "loss: 0.513765  [ 2272/ 3500]\n",
      "loss: 0.444883  [ 2592/ 3500]\n",
      "loss: 0.393634  [ 2912/ 3500]\n",
      "loss: 0.321495  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.597255 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.397828  [   32/ 3500]\n",
      "loss: 0.528736  [  352/ 3500]\n",
      "loss: 0.419804  [  672/ 3500]\n",
      "loss: 0.444496  [  992/ 3500]\n",
      "loss: 0.512000  [ 1312/ 3500]\n",
      "loss: 0.738504  [ 1632/ 3500]\n",
      "loss: 0.426923  [ 1952/ 3500]\n",
      "loss: 0.235125  [ 2272/ 3500]\n",
      "loss: 0.259339  [ 2592/ 3500]\n",
      "loss: 0.560860  [ 2912/ 3500]\n",
      "loss: 0.399901  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.633315 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.412981  [   32/ 3500]\n",
      "loss: 0.430620  [  352/ 3500]\n",
      "loss: 0.589503  [  672/ 3500]\n",
      "loss: 0.405210  [  992/ 3500]\n",
      "loss: 0.393962  [ 1312/ 3500]\n",
      "loss: 0.295250  [ 1632/ 3500]\n",
      "loss: 0.325983  [ 1952/ 3500]\n",
      "loss: 0.432804  [ 2272/ 3500]\n",
      "loss: 0.434713  [ 2592/ 3500]\n",
      "loss: 0.373249  [ 2912/ 3500]\n",
      "loss: 0.398459  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.640448 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.397222  [   32/ 3500]\n",
      "loss: 0.275984  [  352/ 3500]\n",
      "loss: 0.397519  [  672/ 3500]\n",
      "loss: 0.384438  [  992/ 3500]\n",
      "loss: 0.624683  [ 1312/ 3500]\n",
      "loss: 0.433480  [ 1632/ 3500]\n",
      "loss: 0.199495  [ 1952/ 3500]\n",
      "loss: 0.258626  [ 2272/ 3500]\n",
      "loss: 0.307728  [ 2592/ 3500]\n",
      "loss: 0.397416  [ 2912/ 3500]\n",
      "loss: 0.375782  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.692943 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.493580  [   32/ 3500]\n",
      "loss: 0.531723  [  352/ 3500]\n",
      "loss: 0.496341  [  672/ 3500]\n",
      "loss: 0.497169  [  992/ 3500]\n",
      "loss: 0.382255  [ 1312/ 3500]\n",
      "loss: 0.443828  [ 1632/ 3500]\n",
      "loss: 0.503756  [ 1952/ 3500]\n",
      "loss: 0.521680  [ 2272/ 3500]\n",
      "loss: 0.451392  [ 2592/ 3500]\n",
      "loss: 0.501088  [ 2912/ 3500]\n",
      "loss: 0.425075  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.774366 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.430586  [   32/ 3500]\n",
      "loss: 0.182118  [  352/ 3500]\n",
      "loss: 0.346695  [  672/ 3500]\n",
      "loss: 0.638004  [  992/ 3500]\n",
      "loss: 0.439556  [ 1312/ 3500]\n",
      "loss: 0.343306  [ 1632/ 3500]\n",
      "loss: 0.339675  [ 1952/ 3500]\n",
      "loss: 0.410186  [ 2272/ 3500]\n",
      "loss: 0.266844  [ 2592/ 3500]\n",
      "loss: 0.314499  [ 2912/ 3500]\n",
      "loss: 0.313168  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.627827 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.407167  [   32/ 3500]\n",
      "loss: 0.384997  [  352/ 3500]\n",
      "loss: 0.337786  [  672/ 3500]\n",
      "loss: 0.305168  [  992/ 3500]\n",
      "loss: 0.526941  [ 1312/ 3500]\n",
      "loss: 0.334455  [ 1632/ 3500]\n",
      "loss: 0.236548  [ 1952/ 3500]\n",
      "loss: 0.367977  [ 2272/ 3500]\n",
      "loss: 0.455354  [ 2592/ 3500]\n",
      "loss: 0.175989  [ 2912/ 3500]\n",
      "loss: 0.399547  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.750186 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.535740  [   32/ 3500]\n",
      "loss: 0.231660  [  352/ 3500]\n",
      "loss: 0.326209  [  672/ 3500]\n",
      "loss: 0.300938  [  992/ 3500]\n",
      "loss: 0.376225  [ 1312/ 3500]\n",
      "loss: 0.422838  [ 1632/ 3500]\n",
      "loss: 0.396041  [ 1952/ 3500]\n",
      "loss: 0.366315  [ 2272/ 3500]\n",
      "loss: 0.468891  [ 2592/ 3500]\n",
      "loss: 0.353128  [ 2912/ 3500]\n",
      "loss: 0.420049  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.695815 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.264032  [   32/ 3500]\n",
      "loss: 0.410871  [  352/ 3500]\n",
      "loss: 0.198399  [  672/ 3500]\n",
      "loss: 0.214256  [  992/ 3500]\n",
      "loss: 0.338994  [ 1312/ 3500]\n",
      "loss: 0.376130  [ 1632/ 3500]\n",
      "loss: 0.131769  [ 1952/ 3500]\n",
      "loss: 0.389854  [ 2272/ 3500]\n",
      "loss: 0.224424  [ 2592/ 3500]\n",
      "loss: 0.377309  [ 2912/ 3500]\n",
      "loss: 0.272391  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.559568 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.278081  [   32/ 3500]\n",
      "loss: 0.244995  [  352/ 3500]\n",
      "loss: 0.269160  [  672/ 3500]\n",
      "loss: 0.337307  [  992/ 3500]\n",
      "loss: 0.232594  [ 1312/ 3500]\n",
      "loss: 0.390629  [ 1632/ 3500]\n",
      "loss: 0.517752  [ 1952/ 3500]\n",
      "loss: 0.427125  [ 2272/ 3500]\n",
      "loss: 0.184637  [ 2592/ 3500]\n",
      "loss: 0.444224  [ 2912/ 3500]\n",
      "loss: 0.193641  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.609008 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.288736  [   32/ 3500]\n",
      "loss: 0.515189  [  352/ 3500]\n",
      "loss: 0.544079  [  672/ 3500]\n",
      "loss: 0.173784  [  992/ 3500]\n",
      "loss: 0.307103  [ 1312/ 3500]\n",
      "loss: 0.517447  [ 1632/ 3500]\n",
      "loss: 0.230226  [ 1952/ 3500]\n",
      "loss: 0.407769  [ 2272/ 3500]\n",
      "loss: 0.439309  [ 2592/ 3500]\n",
      "loss: 0.208754  [ 2912/ 3500]\n",
      "loss: 0.443219  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.618173 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.396890  [   32/ 3500]\n",
      "loss: 0.247911  [  352/ 3500]\n",
      "loss: 0.355218  [  672/ 3500]\n",
      "loss: 0.309383  [  992/ 3500]\n",
      "loss: 0.327172  [ 1312/ 3500]\n",
      "loss: 0.324712  [ 1632/ 3500]\n",
      "loss: 0.397436  [ 1952/ 3500]\n",
      "loss: 0.293342  [ 2272/ 3500]\n",
      "loss: 0.297695  [ 2592/ 3500]\n",
      "loss: 0.427557  [ 2912/ 3500]\n",
      "loss: 0.297530  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.581297 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.154526  [   32/ 3500]\n",
      "loss: 0.368951  [  352/ 3500]\n",
      "loss: 0.492682  [  672/ 3500]\n",
      "loss: 0.365252  [  992/ 3500]\n",
      "loss: 0.324374  [ 1312/ 3500]\n",
      "loss: 0.301322  [ 1632/ 3500]\n",
      "loss: 0.309664  [ 1952/ 3500]\n",
      "loss: 0.391226  [ 2272/ 3500]\n",
      "loss: 0.440921  [ 2592/ 3500]\n",
      "loss: 0.431532  [ 2912/ 3500]\n",
      "loss: 0.310853  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.633480 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.289261  [   32/ 3500]\n",
      "loss: 0.358849  [  352/ 3500]\n",
      "loss: 0.271902  [  672/ 3500]\n",
      "loss: 0.282621  [  992/ 3500]\n",
      "loss: 0.326263  [ 1312/ 3500]\n",
      "loss: 0.496278  [ 1632/ 3500]\n",
      "loss: 0.269743  [ 1952/ 3500]\n",
      "loss: 0.161924  [ 2272/ 3500]\n",
      "loss: 0.383209  [ 2592/ 3500]\n",
      "loss: 0.334915  [ 2912/ 3500]\n",
      "loss: 0.292740  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.594241 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.385701  [   32/ 3500]\n",
      "loss: 0.386601  [  352/ 3500]\n",
      "loss: 0.287446  [  672/ 3500]\n",
      "loss: 0.517499  [  992/ 3500]\n",
      "loss: 0.355484  [ 1312/ 3500]\n",
      "loss: 0.219487  [ 1632/ 3500]\n",
      "loss: 0.334724  [ 1952/ 3500]\n",
      "loss: 0.283015  [ 2272/ 3500]\n",
      "loss: 0.264462  [ 2592/ 3500]\n",
      "loss: 0.212475  [ 2912/ 3500]\n",
      "loss: 0.245210  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.647955 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.351192  [   32/ 3500]\n",
      "loss: 0.149099  [  352/ 3500]\n",
      "loss: 0.532882  [  672/ 3500]\n",
      "loss: 0.150674  [  992/ 3500]\n",
      "loss: 0.240403  [ 1312/ 3500]\n",
      "loss: 0.381390  [ 1632/ 3500]\n",
      "loss: 0.160142  [ 1952/ 3500]\n",
      "loss: 0.233855  [ 2272/ 3500]\n",
      "loss: 0.541949  [ 2592/ 3500]\n",
      "loss: 0.383342  [ 2912/ 3500]\n",
      "loss: 0.253719  [ 3232/ 3500]\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.586727 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.262318  [   32/ 3500]\n",
      "loss: 0.275534  [  352/ 3500]\n",
      "loss: 0.318173  [  672/ 3500]\n",
      "loss: 0.270072  [  992/ 3500]\n",
      "loss: 0.263902  [ 1312/ 3500]\n",
      "loss: 0.265808  [ 1632/ 3500]\n",
      "loss: 0.338152  [ 1952/ 3500]\n",
      "loss: 0.233313  [ 2272/ 3500]\n",
      "loss: 0.188522  [ 2592/ 3500]\n",
      "loss: 0.330386  [ 2912/ 3500]\n",
      "loss: 0.480084  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.678172 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.288081  [   32/ 3500]\n",
      "loss: 0.229383  [  352/ 3500]\n",
      "loss: 0.357269  [  672/ 3500]\n",
      "loss: 0.343941  [  992/ 3500]\n",
      "loss: 0.230263  [ 1312/ 3500]\n",
      "loss: 0.246861  [ 1632/ 3500]\n",
      "loss: 0.187688  [ 1952/ 3500]\n",
      "loss: 0.350436  [ 2272/ 3500]\n",
      "loss: 0.280568  [ 2592/ 3500]\n",
      "loss: 0.287980  [ 2912/ 3500]\n",
      "loss: 0.289452  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.592129 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.245309  [   32/ 3500]\n",
      "loss: 0.373227  [  352/ 3500]\n",
      "loss: 0.222749  [  672/ 3500]\n",
      "loss: 0.159164  [  992/ 3500]\n",
      "loss: 0.248947  [ 1312/ 3500]\n",
      "loss: 0.260582  [ 1632/ 3500]\n",
      "loss: 0.369335  [ 1952/ 3500]\n",
      "loss: 0.345987  [ 2272/ 3500]\n",
      "loss: 0.305726  [ 2592/ 3500]\n",
      "loss: 0.339812  [ 2912/ 3500]\n",
      "loss: 0.253016  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.578744 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.371326  [   32/ 3500]\n",
      "loss: 0.241106  [  352/ 3500]\n",
      "loss: 0.249084  [  672/ 3500]\n",
      "loss: 0.287708  [  992/ 3500]\n",
      "loss: 0.159361  [ 1312/ 3500]\n",
      "loss: 0.210304  [ 1632/ 3500]\n",
      "loss: 0.138187  [ 1952/ 3500]\n",
      "loss: 0.191785  [ 2272/ 3500]\n",
      "loss: 0.256250  [ 2592/ 3500]\n",
      "loss: 0.409786  [ 2912/ 3500]\n",
      "loss: 0.496158  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.586567 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.333255  [   32/ 3500]\n",
      "loss: 0.422258  [  352/ 3500]\n",
      "loss: 0.284612  [  672/ 3500]\n",
      "loss: 0.200884  [  992/ 3500]\n",
      "loss: 0.449688  [ 1312/ 3500]\n",
      "loss: 0.135185  [ 1632/ 3500]\n",
      "loss: 0.352190  [ 1952/ 3500]\n",
      "loss: 0.314736  [ 2272/ 3500]\n",
      "loss: 0.199622  [ 2592/ 3500]\n",
      "loss: 0.249229  [ 2912/ 3500]\n",
      "loss: 0.344703  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.620600 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.237664  [   32/ 3500]\n",
      "loss: 0.414405  [  352/ 3500]\n",
      "loss: 0.299019  [  672/ 3500]\n",
      "loss: 0.319040  [  992/ 3500]\n",
      "loss: 0.238447  [ 1312/ 3500]\n",
      "loss: 0.359054  [ 1632/ 3500]\n",
      "loss: 0.209997  [ 1952/ 3500]\n",
      "loss: 0.306508  [ 2272/ 3500]\n",
      "loss: 0.410440  [ 2592/ 3500]\n",
      "loss: 0.292321  [ 2912/ 3500]\n",
      "loss: 0.405256  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.613786 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.186626  [   32/ 3500]\n",
      "loss: 0.455533  [  352/ 3500]\n",
      "loss: 0.219382  [  672/ 3500]\n",
      "loss: 0.192745  [  992/ 3500]\n",
      "loss: 0.263274  [ 1312/ 3500]\n",
      "loss: 0.166200  [ 1632/ 3500]\n",
      "loss: 0.354412  [ 1952/ 3500]\n",
      "loss: 0.292618  [ 2272/ 3500]\n",
      "loss: 0.406192  [ 2592/ 3500]\n",
      "loss: 0.281805  [ 2912/ 3500]\n",
      "loss: 0.217335  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.580130 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.208871  [   32/ 3500]\n",
      "loss: 0.147122  [  352/ 3500]\n",
      "loss: 0.220475  [  672/ 3500]\n",
      "loss: 0.247927  [  992/ 3500]\n",
      "loss: 0.156706  [ 1312/ 3500]\n",
      "loss: 0.289807  [ 1632/ 3500]\n",
      "loss: 0.247234  [ 1952/ 3500]\n",
      "loss: 0.229899  [ 2272/ 3500]\n",
      "loss: 0.490014  [ 2592/ 3500]\n",
      "loss: 0.257701  [ 2912/ 3500]\n",
      "loss: 0.408124  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.621872 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.327726  [   32/ 3500]\n",
      "loss: 0.144296  [  352/ 3500]\n",
      "loss: 0.172137  [  672/ 3500]\n",
      "loss: 0.213205  [  992/ 3500]\n",
      "loss: 0.356050  [ 1312/ 3500]\n",
      "loss: 0.206109  [ 1632/ 3500]\n",
      "loss: 0.215691  [ 1952/ 3500]\n",
      "loss: 0.306498  [ 2272/ 3500]\n",
      "loss: 0.397942  [ 2592/ 3500]\n",
      "loss: 0.192089  [ 2912/ 3500]\n",
      "loss: 0.343764  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.737759 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.308918  [   32/ 3500]\n",
      "loss: 0.238225  [  352/ 3500]\n",
      "loss: 0.193718  [  672/ 3500]\n",
      "loss: 0.635087  [  992/ 3500]\n",
      "loss: 0.134341  [ 1312/ 3500]\n",
      "loss: 0.196448  [ 1632/ 3500]\n",
      "loss: 0.212641  [ 1952/ 3500]\n",
      "loss: 0.288955  [ 2272/ 3500]\n",
      "loss: 0.365228  [ 2592/ 3500]\n",
      "loss: 0.410604  [ 2912/ 3500]\n",
      "loss: 0.268141  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.552174 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.199251  [   32/ 3500]\n",
      "loss: 0.341842  [  352/ 3500]\n",
      "loss: 0.277506  [  672/ 3500]\n",
      "loss: 0.255417  [  992/ 3500]\n",
      "loss: 0.148651  [ 1312/ 3500]\n",
      "loss: 0.266813  [ 1632/ 3500]\n",
      "loss: 0.221393  [ 1952/ 3500]\n",
      "loss: 0.391659  [ 2272/ 3500]\n",
      "loss: 0.290195  [ 2592/ 3500]\n",
      "loss: 0.114035  [ 2912/ 3500]\n",
      "loss: 0.206267  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.684454 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.311303  [   32/ 3500]\n",
      "loss: 0.208825  [  352/ 3500]\n",
      "loss: 0.227988  [  672/ 3500]\n",
      "loss: 0.302969  [  992/ 3500]\n",
      "loss: 0.241670  [ 1312/ 3500]\n",
      "loss: 0.406912  [ 1632/ 3500]\n",
      "loss: 0.154024  [ 1952/ 3500]\n",
      "loss: 0.478720  [ 2272/ 3500]\n",
      "loss: 0.222323  [ 2592/ 3500]\n",
      "loss: 0.263421  [ 2912/ 3500]\n",
      "loss: 0.162776  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.670097 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.349457  [   32/ 3500]\n",
      "loss: 0.145767  [  352/ 3500]\n",
      "loss: 0.176556  [  672/ 3500]\n",
      "loss: 0.329974  [  992/ 3500]\n",
      "loss: 0.206955  [ 1312/ 3500]\n",
      "loss: 0.251610  [ 1632/ 3500]\n",
      "loss: 0.157901  [ 1952/ 3500]\n",
      "loss: 0.287485  [ 2272/ 3500]\n",
      "loss: 0.350074  [ 2592/ 3500]\n",
      "loss: 0.208855  [ 2912/ 3500]\n",
      "loss: 0.198474  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.610058 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Epochs = 50, Learning Rate = 1e-5, 3 input -> 32 featueres, 32 input -> 64 features, 64 -> 32 features\n",
    "model_50_1e_5_1 = NeuralNetwork_Conv().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_50_1e_5_1.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 50\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_5_1, loss_fn, optimizer))\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_5_1, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "\n",
    "#Save this model to CNN_50_1e_5_1.pth\n",
    "torch.save(model_50_1e_5_1.state_dict(), \"./CNN_50_1e_5_1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_Conv, self).__init__()\n",
    "        #Images are R=240 by C=360\n",
    "        self.conv_stack = nn.Sequential(\n",
    "           nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        #  Resulting image should be 60*90\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #Padding preserves shape, but 2 max pools divides dims by 4\n",
    "            nn.Linear(60*90*32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc_stack(self.conv_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.990036  [   32/ 3500]\n",
      "loss: 1.520832  [  352/ 3500]\n",
      "loss: 1.428596  [  672/ 3500]\n",
      "loss: 1.393028  [  992/ 3500]\n",
      "loss: 1.393669  [ 1312/ 3500]\n",
      "loss: 1.251886  [ 1632/ 3500]\n",
      "loss: 1.231560  [ 1952/ 3500]\n",
      "loss: 1.338836  [ 2272/ 3500]\n",
      "loss: 1.260650  [ 2592/ 3500]\n",
      "loss: 1.155259  [ 2912/ 3500]\n",
      "loss: 1.167247  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.103982 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 1.120687  [   32/ 3500]\n",
      "loss: 0.969489  [  352/ 3500]\n",
      "loss: 1.235627  [  672/ 3500]\n",
      "loss: 1.081688  [  992/ 3500]\n",
      "loss: 1.038332  [ 1312/ 3500]\n",
      "loss: 0.932425  [ 1632/ 3500]\n",
      "loss: 0.896009  [ 1952/ 3500]\n",
      "loss: 1.019098  [ 2272/ 3500]\n",
      "loss: 1.015104  [ 2592/ 3500]\n",
      "loss: 0.919173  [ 2912/ 3500]\n",
      "loss: 0.862822  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.923497 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.896128  [   32/ 3500]\n",
      "loss: 1.179247  [  352/ 3500]\n",
      "loss: 0.822269  [  672/ 3500]\n",
      "loss: 0.972323  [  992/ 3500]\n",
      "loss: 0.718023  [ 1312/ 3500]\n",
      "loss: 0.927565  [ 1632/ 3500]\n",
      "loss: 0.847958  [ 1952/ 3500]\n",
      "loss: 0.795883  [ 2272/ 3500]\n",
      "loss: 1.016509  [ 2592/ 3500]\n",
      "loss: 1.091812  [ 2912/ 3500]\n",
      "loss: 0.751980  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.857463 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.814231  [   32/ 3500]\n",
      "loss: 0.690597  [  352/ 3500]\n",
      "loss: 0.800503  [  672/ 3500]\n",
      "loss: 0.791503  [  992/ 3500]\n",
      "loss: 0.851392  [ 1312/ 3500]\n",
      "loss: 0.760451  [ 1632/ 3500]\n",
      "loss: 0.692648  [ 1952/ 3500]\n",
      "loss: 0.896001  [ 2272/ 3500]\n",
      "loss: 0.978559  [ 2592/ 3500]\n",
      "loss: 0.776937  [ 2912/ 3500]\n",
      "loss: 1.108812  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.782799 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.716031  [   32/ 3500]\n",
      "loss: 0.808216  [  352/ 3500]\n",
      "loss: 0.842621  [  672/ 3500]\n",
      "loss: 0.604123  [  992/ 3500]\n",
      "loss: 0.686309  [ 1312/ 3500]\n",
      "loss: 0.601786  [ 1632/ 3500]\n",
      "loss: 0.639024  [ 1952/ 3500]\n",
      "loss: 0.920462  [ 2272/ 3500]\n",
      "loss: 0.827244  [ 2592/ 3500]\n",
      "loss: 0.769026  [ 2912/ 3500]\n",
      "loss: 0.827835  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.769526 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.693770  [   32/ 3500]\n",
      "loss: 0.836897  [  352/ 3500]\n",
      "loss: 0.853391  [  672/ 3500]\n",
      "loss: 0.696353  [  992/ 3500]\n",
      "loss: 0.786757  [ 1312/ 3500]\n",
      "loss: 0.623726  [ 1632/ 3500]\n",
      "loss: 0.625255  [ 1952/ 3500]\n",
      "loss: 0.633777  [ 2272/ 3500]\n",
      "loss: 0.881538  [ 2592/ 3500]\n",
      "loss: 0.787907  [ 2912/ 3500]\n",
      "loss: 0.665888  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.727283 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.623509  [   32/ 3500]\n",
      "loss: 0.658179  [  352/ 3500]\n",
      "loss: 0.872632  [  672/ 3500]\n",
      "loss: 0.654693  [  992/ 3500]\n",
      "loss: 0.649535  [ 1312/ 3500]\n",
      "loss: 0.616350  [ 1632/ 3500]\n",
      "loss: 0.833445  [ 1952/ 3500]\n",
      "loss: 0.718088  [ 2272/ 3500]\n",
      "loss: 0.769790  [ 2592/ 3500]\n",
      "loss: 0.464289  [ 2912/ 3500]\n",
      "loss: 0.705299  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.699061 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.870050  [   32/ 3500]\n",
      "loss: 0.732316  [  352/ 3500]\n",
      "loss: 0.597125  [  672/ 3500]\n",
      "loss: 0.597541  [  992/ 3500]\n",
      "loss: 0.721585  [ 1312/ 3500]\n",
      "loss: 0.540412  [ 1632/ 3500]\n",
      "loss: 0.763480  [ 1952/ 3500]\n",
      "loss: 0.352777  [ 2272/ 3500]\n",
      "loss: 0.765998  [ 2592/ 3500]\n",
      "loss: 0.836706  [ 2912/ 3500]\n",
      "loss: 0.602698  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.696596 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.790401  [   32/ 3500]\n",
      "loss: 0.729098  [  352/ 3500]\n",
      "loss: 0.767678  [  672/ 3500]\n",
      "loss: 0.553883  [  992/ 3500]\n",
      "loss: 0.767397  [ 1312/ 3500]\n",
      "loss: 0.575811  [ 1632/ 3500]\n",
      "loss: 0.715235  [ 1952/ 3500]\n",
      "loss: 0.573956  [ 2272/ 3500]\n",
      "loss: 0.818156  [ 2592/ 3500]\n",
      "loss: 0.777963  [ 2912/ 3500]\n",
      "loss: 0.754554  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.684596 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.644273  [   32/ 3500]\n",
      "loss: 0.552173  [  352/ 3500]\n",
      "loss: 0.628162  [  672/ 3500]\n",
      "loss: 0.812990  [  992/ 3500]\n",
      "loss: 0.673941  [ 1312/ 3500]\n",
      "loss: 0.462409  [ 1632/ 3500]\n",
      "loss: 0.769050  [ 1952/ 3500]\n",
      "loss: 0.558964  [ 2272/ 3500]\n",
      "loss: 1.073613  [ 2592/ 3500]\n",
      "loss: 0.453150  [ 2912/ 3500]\n",
      "loss: 0.682136  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.662136 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.634812  [   32/ 3500]\n",
      "loss: 0.779330  [  352/ 3500]\n",
      "loss: 0.449908  [  672/ 3500]\n",
      "loss: 0.659388  [  992/ 3500]\n",
      "loss: 0.532732  [ 1312/ 3500]\n",
      "loss: 0.653288  [ 1632/ 3500]\n",
      "loss: 0.688232  [ 1952/ 3500]\n",
      "loss: 0.393362  [ 2272/ 3500]\n",
      "loss: 0.582670  [ 2592/ 3500]\n",
      "loss: 0.691037  [ 2912/ 3500]\n",
      "loss: 0.401775  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.652199 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.609596  [   32/ 3500]\n",
      "loss: 0.440542  [  352/ 3500]\n",
      "loss: 0.482770  [  672/ 3500]\n",
      "loss: 0.716344  [  992/ 3500]\n",
      "loss: 0.759797  [ 1312/ 3500]\n",
      "loss: 0.388949  [ 1632/ 3500]\n",
      "loss: 0.861165  [ 1952/ 3500]\n",
      "loss: 0.883410  [ 2272/ 3500]\n",
      "loss: 0.511524  [ 2592/ 3500]\n",
      "loss: 0.811187  [ 2912/ 3500]\n",
      "loss: 0.722446  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.715643 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.671279  [   32/ 3500]\n",
      "loss: 0.483327  [  352/ 3500]\n",
      "loss: 0.667931  [  672/ 3500]\n",
      "loss: 0.768014  [  992/ 3500]\n",
      "loss: 0.797702  [ 1312/ 3500]\n",
      "loss: 0.691761  [ 1632/ 3500]\n",
      "loss: 0.438164  [ 1952/ 3500]\n",
      "loss: 0.636122  [ 2272/ 3500]\n",
      "loss: 0.639625  [ 2592/ 3500]\n",
      "loss: 0.432213  [ 2912/ 3500]\n",
      "loss: 0.523452  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.657662 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.769298  [   32/ 3500]\n",
      "loss: 0.442160  [  352/ 3500]\n",
      "loss: 0.726860  [  672/ 3500]\n",
      "loss: 0.530609  [  992/ 3500]\n",
      "loss: 0.807877  [ 1312/ 3500]\n",
      "loss: 0.598684  [ 1632/ 3500]\n",
      "loss: 0.896472  [ 1952/ 3500]\n",
      "loss: 0.596361  [ 2272/ 3500]\n",
      "loss: 0.607000  [ 2592/ 3500]\n",
      "loss: 0.439396  [ 2912/ 3500]\n",
      "loss: 0.586739  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.634012 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.335054  [   32/ 3500]\n",
      "loss: 0.750882  [  352/ 3500]\n",
      "loss: 0.818138  [  672/ 3500]\n",
      "loss: 0.748888  [  992/ 3500]\n",
      "loss: 0.601739  [ 1312/ 3500]\n",
      "loss: 0.671256  [ 1632/ 3500]\n",
      "loss: 0.803852  [ 1952/ 3500]\n",
      "loss: 0.520662  [ 2272/ 3500]\n",
      "loss: 0.575642  [ 2592/ 3500]\n",
      "loss: 0.670280  [ 2912/ 3500]\n",
      "loss: 0.359464  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.651287 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.640449  [   32/ 3500]\n",
      "loss: 0.778299  [  352/ 3500]\n",
      "loss: 0.507133  [  672/ 3500]\n",
      "loss: 0.476360  [  992/ 3500]\n",
      "loss: 0.531371  [ 1312/ 3500]\n",
      "loss: 0.757989  [ 1632/ 3500]\n",
      "loss: 0.488805  [ 1952/ 3500]\n",
      "loss: 0.545331  [ 2272/ 3500]\n",
      "loss: 0.679994  [ 2592/ 3500]\n",
      "loss: 0.678164  [ 2912/ 3500]\n",
      "loss: 0.691982  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.635366 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.675015  [   32/ 3500]\n",
      "loss: 0.550542  [  352/ 3500]\n",
      "loss: 0.390514  [  672/ 3500]\n",
      "loss: 0.451658  [  992/ 3500]\n",
      "loss: 0.486650  [ 1312/ 3500]\n",
      "loss: 0.818320  [ 1632/ 3500]\n",
      "loss: 0.677434  [ 1952/ 3500]\n",
      "loss: 0.706539  [ 2272/ 3500]\n",
      "loss: 0.529445  [ 2592/ 3500]\n",
      "loss: 0.516854  [ 2912/ 3500]\n",
      "loss: 0.497524  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.616074 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.511636  [   32/ 3500]\n",
      "loss: 0.420127  [  352/ 3500]\n",
      "loss: 0.680393  [  672/ 3500]\n",
      "loss: 0.617439  [  992/ 3500]\n",
      "loss: 0.654204  [ 1312/ 3500]\n",
      "loss: 0.432918  [ 1632/ 3500]\n",
      "loss: 0.580956  [ 1952/ 3500]\n",
      "loss: 0.409304  [ 2272/ 3500]\n",
      "loss: 0.309313  [ 2592/ 3500]\n",
      "loss: 0.581578  [ 2912/ 3500]\n",
      "loss: 0.542285  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.604383 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.462421  [   32/ 3500]\n",
      "loss: 0.800555  [  352/ 3500]\n",
      "loss: 0.563253  [  672/ 3500]\n",
      "loss: 0.458601  [  992/ 3500]\n",
      "loss: 0.929130  [ 1312/ 3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.333025  [ 1632/ 3500]\n",
      "loss: 0.684506  [ 1952/ 3500]\n",
      "loss: 0.831792  [ 2272/ 3500]\n",
      "loss: 0.752620  [ 2592/ 3500]\n",
      "loss: 0.639184  [ 2912/ 3500]\n",
      "loss: 0.423371  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.624212 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.466448  [   32/ 3500]\n",
      "loss: 0.473728  [  352/ 3500]\n",
      "loss: 0.647576  [  672/ 3500]\n",
      "loss: 0.790029  [  992/ 3500]\n",
      "loss: 0.656370  [ 1312/ 3500]\n",
      "loss: 0.471537  [ 1632/ 3500]\n",
      "loss: 0.579885  [ 1952/ 3500]\n",
      "loss: 0.594476  [ 2272/ 3500]\n",
      "loss: 0.551721  [ 2592/ 3500]\n",
      "loss: 0.347955  [ 2912/ 3500]\n",
      "loss: 0.478053  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.589668 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.525101  [   32/ 3500]\n",
      "loss: 0.298786  [  352/ 3500]\n",
      "loss: 0.543531  [  672/ 3500]\n",
      "loss: 0.630624  [  992/ 3500]\n",
      "loss: 0.473268  [ 1312/ 3500]\n",
      "loss: 0.526629  [ 1632/ 3500]\n",
      "loss: 0.429073  [ 1952/ 3500]\n",
      "loss: 0.346382  [ 2272/ 3500]\n",
      "loss: 0.348920  [ 2592/ 3500]\n",
      "loss: 0.435855  [ 2912/ 3500]\n",
      "loss: 0.478418  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.624690 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.595713  [   32/ 3500]\n",
      "loss: 0.443992  [  352/ 3500]\n",
      "loss: 0.706205  [  672/ 3500]\n",
      "loss: 0.633562  [  992/ 3500]\n",
      "loss: 0.573337  [ 1312/ 3500]\n",
      "loss: 0.442282  [ 1632/ 3500]\n",
      "loss: 0.431559  [ 1952/ 3500]\n",
      "loss: 0.454411  [ 2272/ 3500]\n",
      "loss: 0.500741  [ 2592/ 3500]\n",
      "loss: 0.556831  [ 2912/ 3500]\n",
      "loss: 0.686484  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.589787 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.490806  [   32/ 3500]\n",
      "loss: 0.469397  [  352/ 3500]\n",
      "loss: 0.391524  [  672/ 3500]\n",
      "loss: 0.401381  [  992/ 3500]\n",
      "loss: 0.359821  [ 1312/ 3500]\n",
      "loss: 0.412989  [ 1632/ 3500]\n",
      "loss: 0.656216  [ 1952/ 3500]\n",
      "loss: 0.574440  [ 2272/ 3500]\n",
      "loss: 0.691424  [ 2592/ 3500]\n",
      "loss: 0.613963  [ 2912/ 3500]\n",
      "loss: 0.523972  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.595323 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.539938  [   32/ 3500]\n",
      "loss: 0.471150  [  352/ 3500]\n",
      "loss: 0.367160  [  672/ 3500]\n",
      "loss: 0.409540  [  992/ 3500]\n",
      "loss: 0.476809  [ 1312/ 3500]\n",
      "loss: 0.438349  [ 1632/ 3500]\n",
      "loss: 0.452193  [ 1952/ 3500]\n",
      "loss: 0.722288  [ 2272/ 3500]\n",
      "loss: 0.481345  [ 2592/ 3500]\n",
      "loss: 0.442546  [ 2912/ 3500]\n",
      "loss: 0.571217  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.600225 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.331089  [   32/ 3500]\n",
      "loss: 0.701304  [  352/ 3500]\n",
      "loss: 0.537843  [  672/ 3500]\n",
      "loss: 0.322276  [  992/ 3500]\n",
      "loss: 0.636814  [ 1312/ 3500]\n",
      "loss: 0.455150  [ 1632/ 3500]\n",
      "loss: 0.303024  [ 1952/ 3500]\n",
      "loss: 0.577629  [ 2272/ 3500]\n",
      "loss: 0.503233  [ 2592/ 3500]\n",
      "loss: 0.479952  [ 2912/ 3500]\n",
      "loss: 0.427447  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.581759 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.559066  [   32/ 3500]\n",
      "loss: 0.539307  [  352/ 3500]\n",
      "loss: 0.543167  [  672/ 3500]\n",
      "loss: 0.547324  [  992/ 3500]\n",
      "loss: 0.438542  [ 1312/ 3500]\n",
      "loss: 0.391487  [ 1632/ 3500]\n",
      "loss: 0.709036  [ 1952/ 3500]\n",
      "loss: 0.375347  [ 2272/ 3500]\n",
      "loss: 0.372791  [ 2592/ 3500]\n",
      "loss: 0.372140  [ 2912/ 3500]\n",
      "loss: 0.521517  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.604675 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.485233  [   32/ 3500]\n",
      "loss: 0.513674  [  352/ 3500]\n",
      "loss: 0.649166  [  672/ 3500]\n",
      "loss: 0.593288  [  992/ 3500]\n",
      "loss: 0.474453  [ 1312/ 3500]\n",
      "loss: 0.425937  [ 1632/ 3500]\n",
      "loss: 0.660252  [ 1952/ 3500]\n",
      "loss: 0.347761  [ 2272/ 3500]\n",
      "loss: 0.587350  [ 2592/ 3500]\n",
      "loss: 0.554863  [ 2912/ 3500]\n",
      "loss: 0.330826  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.651379 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.731218  [   32/ 3500]\n",
      "loss: 0.538228  [  352/ 3500]\n",
      "loss: 0.355720  [  672/ 3500]\n",
      "loss: 0.462902  [  992/ 3500]\n",
      "loss: 0.689173  [ 1312/ 3500]\n",
      "loss: 0.363631  [ 1632/ 3500]\n",
      "loss: 0.375517  [ 1952/ 3500]\n",
      "loss: 0.303778  [ 2272/ 3500]\n",
      "loss: 0.325864  [ 2592/ 3500]\n",
      "loss: 0.298092  [ 2912/ 3500]\n",
      "loss: 0.566206  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.627428 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.550225  [   32/ 3500]\n",
      "loss: 0.404930  [  352/ 3500]\n",
      "loss: 0.333560  [  672/ 3500]\n",
      "loss: 0.377897  [  992/ 3500]\n",
      "loss: 0.314348  [ 1312/ 3500]\n",
      "loss: 0.439429  [ 1632/ 3500]\n",
      "loss: 0.524925  [ 1952/ 3500]\n",
      "loss: 0.498976  [ 2272/ 3500]\n",
      "loss: 0.524064  [ 2592/ 3500]\n",
      "loss: 0.672249  [ 2912/ 3500]\n",
      "loss: 0.388161  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.576802 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.522144  [   32/ 3500]\n",
      "loss: 0.611606  [  352/ 3500]\n",
      "loss: 0.586714  [  672/ 3500]\n",
      "loss: 0.465361  [  992/ 3500]\n",
      "loss: 0.527912  [ 1312/ 3500]\n",
      "loss: 0.354496  [ 1632/ 3500]\n",
      "loss: 0.549835  [ 1952/ 3500]\n",
      "loss: 0.464794  [ 2272/ 3500]\n",
      "loss: 0.598250  [ 2592/ 3500]\n",
      "loss: 0.334178  [ 2912/ 3500]\n",
      "loss: 0.567073  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.580636 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.229221  [   32/ 3500]\n",
      "loss: 0.366527  [  352/ 3500]\n",
      "loss: 0.402941  [  672/ 3500]\n",
      "loss: 0.397072  [  992/ 3500]\n",
      "loss: 0.371672  [ 1312/ 3500]\n",
      "loss: 0.484097  [ 1632/ 3500]\n",
      "loss: 0.404621  [ 1952/ 3500]\n",
      "loss: 0.507365  [ 2272/ 3500]\n",
      "loss: 0.537229  [ 2592/ 3500]\n",
      "loss: 0.624958  [ 2912/ 3500]\n",
      "loss: 0.346073  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.598018 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.450777  [   32/ 3500]\n",
      "loss: 0.417049  [  352/ 3500]\n",
      "loss: 0.289622  [  672/ 3500]\n",
      "loss: 0.338205  [  992/ 3500]\n",
      "loss: 0.554719  [ 1312/ 3500]\n",
      "loss: 0.295203  [ 1632/ 3500]\n",
      "loss: 0.370211  [ 1952/ 3500]\n",
      "loss: 0.743626  [ 2272/ 3500]\n",
      "loss: 0.456133  [ 2592/ 3500]\n",
      "loss: 0.860764  [ 2912/ 3500]\n",
      "loss: 0.607454  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.572018 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.562766  [   32/ 3500]\n",
      "loss: 0.638530  [  352/ 3500]\n",
      "loss: 0.342891  [  672/ 3500]\n",
      "loss: 0.670638  [  992/ 3500]\n",
      "loss: 0.542356  [ 1312/ 3500]\n",
      "loss: 0.494758  [ 1632/ 3500]\n",
      "loss: 0.664165  [ 1952/ 3500]\n",
      "loss: 0.510243  [ 2272/ 3500]\n",
      "loss: 0.570811  [ 2592/ 3500]\n",
      "loss: 0.449298  [ 2912/ 3500]\n",
      "loss: 0.254242  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.579173 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.339188  [   32/ 3500]\n",
      "loss: 0.769195  [  352/ 3500]\n",
      "loss: 0.439692  [  672/ 3500]\n",
      "loss: 0.532335  [  992/ 3500]\n",
      "loss: 0.274249  [ 1312/ 3500]\n",
      "loss: 0.353939  [ 1632/ 3500]\n",
      "loss: 0.639988  [ 1952/ 3500]\n",
      "loss: 0.446084  [ 2272/ 3500]\n",
      "loss: 0.405724  [ 2592/ 3500]\n",
      "loss: 0.434167  [ 2912/ 3500]\n",
      "loss: 0.410095  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.586965 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.414549  [   32/ 3500]\n",
      "loss: 0.575220  [  352/ 3500]\n",
      "loss: 0.596286  [  672/ 3500]\n",
      "loss: 0.401883  [  992/ 3500]\n",
      "loss: 0.250491  [ 1312/ 3500]\n",
      "loss: 0.415777  [ 1632/ 3500]\n",
      "loss: 0.329275  [ 1952/ 3500]\n",
      "loss: 0.502132  [ 2272/ 3500]\n",
      "loss: 0.570441  [ 2592/ 3500]\n",
      "loss: 0.511496  [ 2912/ 3500]\n",
      "loss: 0.542132  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.587654 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.476761  [   32/ 3500]\n",
      "loss: 0.236249  [  352/ 3500]\n",
      "loss: 0.291030  [  672/ 3500]\n",
      "loss: 0.397942  [  992/ 3500]\n",
      "loss: 0.426556  [ 1312/ 3500]\n",
      "loss: 0.427146  [ 1632/ 3500]\n",
      "loss: 0.441370  [ 1952/ 3500]\n",
      "loss: 0.325016  [ 2272/ 3500]\n",
      "loss: 0.310009  [ 2592/ 3500]\n",
      "loss: 0.384443  [ 2912/ 3500]\n",
      "loss: 0.292612  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.554887 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.639737  [   32/ 3500]\n",
      "loss: 0.380653  [  352/ 3500]\n",
      "loss: 0.264378  [  672/ 3500]\n",
      "loss: 0.424655  [  992/ 3500]\n",
      "loss: 0.458037  [ 1312/ 3500]\n",
      "loss: 0.304149  [ 1632/ 3500]\n",
      "loss: 0.299868  [ 1952/ 3500]\n",
      "loss: 0.381512  [ 2272/ 3500]\n",
      "loss: 0.407286  [ 2592/ 3500]\n",
      "loss: 0.675360  [ 2912/ 3500]\n",
      "loss: 0.751620  [ 3232/ 3500]\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.591376 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.429960  [   32/ 3500]\n",
      "loss: 0.689739  [  352/ 3500]\n",
      "loss: 0.383062  [  672/ 3500]\n",
      "loss: 0.580111  [  992/ 3500]\n",
      "loss: 0.226452  [ 1312/ 3500]\n",
      "loss: 0.309427  [ 1632/ 3500]\n",
      "loss: 0.541990  [ 1952/ 3500]\n",
      "loss: 0.332090  [ 2272/ 3500]\n",
      "loss: 0.293895  [ 2592/ 3500]\n",
      "loss: 0.252455  [ 2912/ 3500]\n",
      "loss: 0.383452  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.550111 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.207349  [   32/ 3500]\n",
      "loss: 0.328671  [  352/ 3500]\n",
      "loss: 0.529637  [  672/ 3500]\n",
      "loss: 0.719251  [  992/ 3500]\n",
      "loss: 0.407883  [ 1312/ 3500]\n",
      "loss: 0.277318  [ 1632/ 3500]\n",
      "loss: 0.426070  [ 1952/ 3500]\n",
      "loss: 0.413652  [ 2272/ 3500]\n",
      "loss: 0.335228  [ 2592/ 3500]\n",
      "loss: 0.396287  [ 2912/ 3500]\n",
      "loss: 0.572165  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.551646 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.397841  [   32/ 3500]\n",
      "loss: 0.305282  [  352/ 3500]\n",
      "loss: 0.361377  [  672/ 3500]\n",
      "loss: 0.224792  [  992/ 3500]\n",
      "loss: 0.325125  [ 1312/ 3500]\n",
      "loss: 0.192697  [ 1632/ 3500]\n",
      "loss: 0.457976  [ 1952/ 3500]\n",
      "loss: 0.400782  [ 2272/ 3500]\n",
      "loss: 0.398305  [ 2592/ 3500]\n",
      "loss: 0.283634  [ 2912/ 3500]\n",
      "loss: 0.406223  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.579569 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.439148  [   32/ 3500]\n",
      "loss: 0.320942  [  352/ 3500]\n",
      "loss: 0.467062  [  672/ 3500]\n",
      "loss: 0.301576  [  992/ 3500]\n",
      "loss: 0.500696  [ 1312/ 3500]\n",
      "loss: 0.348823  [ 1632/ 3500]\n",
      "loss: 0.561904  [ 1952/ 3500]\n",
      "loss: 0.361950  [ 2272/ 3500]\n",
      "loss: 0.209904  [ 2592/ 3500]\n",
      "loss: 0.456111  [ 2912/ 3500]\n",
      "loss: 0.690533  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.539230 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.484105  [   32/ 3500]\n",
      "loss: 0.305732  [  352/ 3500]\n",
      "loss: 0.636455  [  672/ 3500]\n",
      "loss: 0.481881  [  992/ 3500]\n",
      "loss: 0.374934  [ 1312/ 3500]\n",
      "loss: 0.523788  [ 1632/ 3500]\n",
      "loss: 0.326129  [ 1952/ 3500]\n",
      "loss: 0.176667  [ 2272/ 3500]\n",
      "loss: 0.471658  [ 2592/ 3500]\n",
      "loss: 0.445437  [ 2912/ 3500]\n",
      "loss: 0.392205  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.548120 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.641769  [   32/ 3500]\n",
      "loss: 0.460634  [  352/ 3500]\n",
      "loss: 0.536755  [  672/ 3500]\n",
      "loss: 0.422363  [  992/ 3500]\n",
      "loss: 0.326703  [ 1312/ 3500]\n",
      "loss: 0.395502  [ 1632/ 3500]\n",
      "loss: 0.355586  [ 1952/ 3500]\n",
      "loss: 0.430073  [ 2272/ 3500]\n",
      "loss: 0.383244  [ 2592/ 3500]\n",
      "loss: 0.444276  [ 2912/ 3500]\n",
      "loss: 0.434168  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.551288 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.466737  [   32/ 3500]\n",
      "loss: 0.259602  [  352/ 3500]\n",
      "loss: 0.444193  [  672/ 3500]\n",
      "loss: 0.313824  [  992/ 3500]\n",
      "loss: 0.217594  [ 1312/ 3500]\n",
      "loss: 0.289779  [ 1632/ 3500]\n",
      "loss: 0.505345  [ 1952/ 3500]\n",
      "loss: 0.343052  [ 2272/ 3500]\n",
      "loss: 0.390521  [ 2592/ 3500]\n",
      "loss: 0.297454  [ 2912/ 3500]\n",
      "loss: 0.490112  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.551644 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.472616  [   32/ 3500]\n",
      "loss: 0.364720  [  352/ 3500]\n",
      "loss: 0.341129  [  672/ 3500]\n",
      "loss: 0.311711  [  992/ 3500]\n",
      "loss: 0.512741  [ 1312/ 3500]\n",
      "loss: 0.184473  [ 1632/ 3500]\n",
      "loss: 0.255851  [ 1952/ 3500]\n",
      "loss: 0.379725  [ 2272/ 3500]\n",
      "loss: 0.364807  [ 2592/ 3500]\n",
      "loss: 0.535492  [ 2912/ 3500]\n",
      "loss: 0.546875  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.557100 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.498648  [   32/ 3500]\n",
      "loss: 0.421854  [  352/ 3500]\n",
      "loss: 0.439257  [  672/ 3500]\n",
      "loss: 0.469743  [  992/ 3500]\n",
      "loss: 0.307557  [ 1312/ 3500]\n",
      "loss: 0.389949  [ 1632/ 3500]\n",
      "loss: 0.386893  [ 1952/ 3500]\n",
      "loss: 0.351484  [ 2272/ 3500]\n",
      "loss: 0.284937  [ 2592/ 3500]\n",
      "loss: 0.494040  [ 2912/ 3500]\n",
      "loss: 0.301989  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.543567 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.313990  [   32/ 3500]\n",
      "loss: 0.449599  [  352/ 3500]\n",
      "loss: 0.455720  [  672/ 3500]\n",
      "loss: 0.365215  [  992/ 3500]\n",
      "loss: 0.424707  [ 1312/ 3500]\n",
      "loss: 0.348890  [ 1632/ 3500]\n",
      "loss: 0.182754  [ 1952/ 3500]\n",
      "loss: 0.398872  [ 2272/ 3500]\n",
      "loss: 0.313504  [ 2592/ 3500]\n",
      "loss: 0.267656  [ 2912/ 3500]\n",
      "loss: 0.289836  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.548970 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.372317  [   32/ 3500]\n",
      "loss: 0.336829  [  352/ 3500]\n",
      "loss: 0.333067  [  672/ 3500]\n",
      "loss: 0.256401  [  992/ 3500]\n",
      "loss: 0.557563  [ 1312/ 3500]\n",
      "loss: 0.334488  [ 1632/ 3500]\n",
      "loss: 0.286011  [ 1952/ 3500]\n",
      "loss: 0.429651  [ 2272/ 3500]\n",
      "loss: 0.164936  [ 2592/ 3500]\n",
      "loss: 0.583007  [ 2912/ 3500]\n",
      "loss: 0.279867  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.533384 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.411191  [   32/ 3500]\n",
      "loss: 0.364360  [  352/ 3500]\n",
      "loss: 0.546720  [  672/ 3500]\n",
      "loss: 0.448990  [  992/ 3500]\n",
      "loss: 0.242979  [ 1312/ 3500]\n",
      "loss: 0.419382  [ 1632/ 3500]\n",
      "loss: 0.178530  [ 1952/ 3500]\n",
      "loss: 0.496781  [ 2272/ 3500]\n",
      "loss: 0.337895  [ 2592/ 3500]\n",
      "loss: 0.267276  [ 2912/ 3500]\n",
      "loss: 0.256587  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.527727 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training:\n",
      "loss: 0.316148  [   32/ 3500]\n",
      "loss: 0.588871  [  352/ 3500]\n",
      "loss: 0.209132  [  672/ 3500]\n",
      "loss: 0.302224  [  992/ 3500]\n",
      "loss: 0.251184  [ 1312/ 3500]\n",
      "loss: 0.392348  [ 1632/ 3500]\n",
      "loss: 0.258348  [ 1952/ 3500]\n",
      "loss: 0.286657  [ 2272/ 3500]\n",
      "loss: 0.300992  [ 2592/ 3500]\n",
      "loss: 0.419162  [ 2912/ 3500]\n",
      "loss: 0.394430  [ 3232/ 3500]\n",
      "Validation:\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.550064 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Epochs = 50, Learning Rate = 1e-5, 3 input -> 32 featueres, 32 input -> 64 features, 64 -> 32 features, additional hidden layer for NN\n",
    "model_50_1e_5_2 = NeuralNetwork_Conv().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_50_1e_5_2.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 50\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_5_2, loss_fn, optimizer))\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_5_2, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "\n",
    "#Save this model to CNN_50_1e_5_2.pth\n",
    "torch.save(model_50_1e_5_2.state_dict(), \"./CNN_50_1e_5_2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork_Conv, self).__init__()\n",
    "        #Images are R=240 by C=360\n",
    "        self.conv_stack = nn.Sequential(\n",
    "           nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.MaxPool2d(2,2),\n",
    "           nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        #  Resulting image should be 60*90\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(60*90*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc_stack(self.conv_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 676.00 MiB (GPU 0; 4.00 GiB total capacity; 2.16 GiB already allocated; 0 bytes free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_62036/2222161683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtraining_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_50_1e_5_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_50_1e_5_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_62036/2358757410.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Compute prediction error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VEDANT~1\\AppData\\Local\\Temp/ipykernel_62036/161747642.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\CompVisEnv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 676.00 MiB (GPU 0; 4.00 GiB total capacity; 2.16 GiB already allocated; 0 bytes free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Epochs = 50, Learning Rate = 1e-5, 3 input -> 32 featueres, 32 input -> 64 features, 64 -> 128 features, 128 -> 64 features stride=1\n",
    "#additional hidden layer for NN\n",
    "model_50_1e_5_3 = NeuralNetwork_Conv().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_50_1e_5_3.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 50\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Training:\")\n",
    "    training_losses.append(train(training_dataloader, model_50_1e_5_3, loss_fn, optimizer))\n",
    "    print(\"Validation:\")\n",
    "    validation_losses.append(test(validation_dataloader, model_50_1e_5_3, loss_fn)[0])\n",
    "print(\"Done!\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), validation_losses, marker='o', color='r', label=\"Validation\")\n",
    "plt.plot(np.linspace(1, epochs, epochs), training_losses, marker='o', color='b', label=\"Training\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "\n",
    "#Save this model to CNN_50_1e_5_3.pth\n",
    "torch.save(model_50_1e_5_3.state_dict(), \"./CNN_50_1e_5_3.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.735012 \n",
      "\n",
      "[[ 94   4   0  18  34]\n",
      " [  3  93   3  37  14]\n",
      " [  1   1 133  10   5]\n",
      " [  4   5   6 125  10]\n",
      " [ 11   5   2  12 120]]\n"
     ]
    }
   ],
   "source": [
    "#Applying CNN_50_1e_5 to testing data\n",
    "model_50_1e_5_loaded = NeuralNetwork_Conv().to(device)\n",
    "\n",
    "model_50_1e_5_loaded.load_state_dict(torch.load(\"./CNN_50_1e_5.pth\"\n",
    "                                      ,map_location=device))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "_ , pred_vs_actual = test(testing_dataloader, model_50_1e_5_loaded, loss_fn)\n",
    "print(confusion_matrix(pred_vs_actual.cpu().numpy()[:,1], pred_vs_actual.cpu().numpy()[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.641141 \n",
      "\n",
      "[[106   8   1  18  17]\n",
      " [  3 107   1  27  12]\n",
      " [  1   1 140   7   1]\n",
      " [  8   8   3 116  15]\n",
      " [ 18   8   3  16 105]]\n"
     ]
    }
   ],
   "source": [
    "#Applying CNN_50_1e_5_2 to testing data\n",
    "model_50_1e_5_2_loaded = NeuralNetwork_Conv().to(device)\n",
    "\n",
    "model_50_1e_5_2_loaded.load_state_dict(torch.load(\"./CNN_50_1e_5_2.pth\"\n",
    "                                      ,map_location=device))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "_ , pred_vs_actual = test(testing_dataloader, model_50_1e_5_2_loaded, loss_fn)\n",
    "print(confusion_matrix(pred_vs_actual.cpu().numpy()[:,1], pred_vs_actual.cpu().numpy()[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.642606 \n",
      "\n",
      "[[120   4   1  11  14]\n",
      " [ 13  98   2  30   7]\n",
      " [  1   0 138  10   1]\n",
      " [ 14   6   2 118  10]\n",
      " [ 30   3   4  12 101]]\n"
     ]
    }
   ],
   "source": [
    "#Applying CNN_50_1e_5_3 to testing data\n",
    "model_50_1e_5_3_loaded = NeuralNetwork_Conv().to(device)\n",
    "\n",
    "model_50_1e_5_3_loaded.load_state_dict(torch.load(\"./CNN_50_1e_5_3.pth\"\n",
    "                                      ,map_location=device))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "_ , pred_vs_actual = test(testing_dataloader, model_50_1e_5_3_loaded, loss_fn)\n",
    "print(confusion_matrix(pred_vs_actual.cpu().numpy()[:,1], pred_vs_actual.cpu().numpy()[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMruclEGsIRAmydocI+1soc",
   "collapsed_sections": [],
   "name": "hw5_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
